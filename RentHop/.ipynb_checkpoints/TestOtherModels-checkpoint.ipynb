{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgzuke/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.metrics import log_loss\n",
    "import string\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from scipy.stats import boxcox\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class NBatchLogger(Callback):\n",
    "    def __init__(self):\n",
    "        self.seen = 0\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.seen += 1\n",
    "        if self.seen % 20 == 0:\n",
    "            print('epoch {}: loss = {}, val = {}'.format(self.seen, logs['loss'], logs['val_loss'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = pd.read_json('data/train_scaled_with_leaky_and_added_features_cv.json')\n",
    "train_y = pd.read_json('data/train_interest.json')[0]\n",
    "columns = train_X.columns\n",
    "train_X_scaled = pd.DataFrame(StandardScaler().fit_transform(train_X), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_usefulness = [('bc_price', 16597), ('manager_worths', 14684), ('price_by_NTACode', 13343), ('latitude', 12299), ('longitude', 12051), ('desc_words_length', 11477), ('manager_id', 11136), ('price_by_address1', 11125), ('Yday', 10884), ('address1', 10458), ('building_worths', 10298), ('building_id', 10142), ('desc_letters_count', 9832), ('real_minus_expected_price', 9815), ('price_by_area', 9742), ('real_over_expected_price', 9374), ('Day', 9066), ('worth_by_address1', 8782), ('hour', 7722), ('price_per_bathroom', 7590), ('desc_words_count', 7481), ('price_per_bed_and_bath', 7387), ('worth_by_NTACode', 6993), ('photos_count', 6730), ('NTACode', 6324), ('price_by_BoroCode', 6098), ('features_count', 5668), ('Wday', 4445), ('price_per_bedroom', 3903), ('sparse_nofee', 3056), ('bedrooms', 2833), ('sparse_furnished', 1865), ('bathrooms', 1702), ('sparse_laundryinbuilding', 1425), ('sparse_hardwoodfloors', 1132), ('sparse_laundryinunit', 1088), ('Month', 1006), ('sparse_exclusive', 771), ('sparse_dogsallowed', 770), ('BoroCode', 749), ('sparse_catsallowed', 718), ('sparse_prewar', 663), ('sparse_privateoutdoorspace', 634), ('sparse_reducedfee', 631), ('avenue', 595), ('street', 569), ('other_address', 553), ('sparse_commonoutdoorspace', 545), ('Zero_building_id', 534), ('sparse_elevator', 533), ('east', 524), ('sparse_doorman', 510), ('sparse_parkingspace', 506), ('sparse_dishwasher', 500), ('sparse_highspeedinternet', 476), ('sparse_terrace', 424), ('sparse_fitnesscenter', 423), ('sparse_loft', 413), ('sparse_outdoorspace', 406), ('sparse_actualapt', 399), ('sparse_roofdeck', 359), ('sparse_balcony', 346), ('sparse_diningroom', 318), ('sparse_simplex', 292), ('sparse_swimmingpool', 283), ('sparse_shorttermallowed', 263), ('sparse_wheelchairaccess', 248), ('west', 245), ('sparse_newconstruction', 238), ('sparse_garden', 231), ('sparse_multilevel', 201), ('sparse_patio', 199), ('sparse_hardwood', 198), ('worth_by_BoroCode', 179), ('sparse_fireplace', 168), ('sparse_highceiling', 159), ('sparse_petsok', 154), ('sparse_dryerinunit', 153), ('sparse_stainlesssteelappliances', 148), ('sparse_garage', 119), ('south', 118), ('sparse_storage', 118), ('sparse_outdoorareas', 117), ('sparse_lndrybldg', 113), ('sparse_liveinsuper', 110), ('sparse_new', 103), ('sparse_renovated', 99), ('sparse_photos', 88), ('sparse_highceilings', 79), ('sparse_onsitelaundry', 79), ('sparse_view', 78), ('sparse_flex3', 67), ('sparse_centrala', 66), ('sparse_concierge', 64), ('sparse_residentslounge', 63), ('sparse_assignedparkingspace', 62), ('sparse_allutilitiesincluded', 58), ('sparse_onsitegarage', 53), ('north', 46), ('sparse_lowrise', 44), ('sparse_washerinunit', 44)]\n",
    "leaky = ['manager_worths', 'building_worths', 'price_by_area', 'real_minus_expected_price', 'real_over_expected_price', 'price_by_address1', 'worth_by_address1', 'price_by_BoroCode', 'worth_by_BoroCode', 'price_by_NTACode', 'worth_by_NTACode']\n",
    "sparse_cols = [x[0] for x in column_usefulness if x[0].startswith('sparse')][:15]\n",
    "non_sparse_cols = [x[0] for x in column_usefulness if not x[0].startswith('sparse')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a bunch of features to train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_features = [x[0] for x in column_usefulness[:29]]\n",
    "n_components = 5\n",
    "new_columns_generated = ['k_means'] + ['pca_{}'.format(i) for i in range(n_components)] + ['knn_{}'.format(i) for i in [2, 4, 8, 16, 64, 256, 1024]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column in new_columns_generated:\n",
    "    train_X_scaled[column] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 8.51 s, total: 1min 12s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tsne = TSNE(n_components = 2)\n",
    "tsne_representation = tsne.fit_transform(train_X_scaled[knn_features[:15]][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.88 s, sys: 450 ms, total: 2.33 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pca_cols = ['pca_{}'.format(i) for i in range(n_components)]\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_representation = pca.fit_transform(train_X_scaled)\n",
    "train_X_scaled.loc[:, pca_cols] = pca_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "CPU times: user 27min 13s, sys: 9.89 s, total: 27min 23s\n",
      "Wall time: 27min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "folds = 5\n",
    "full_length = len(train_X_scaled)\n",
    "entries = int(full_length / folds)\n",
    "full_index = train_X_scaled.index\n",
    "fold_indices = []\n",
    "fold_indices_keep = []\n",
    "fold_indices_throw = []\n",
    "for column in leaky:\n",
    "    train_X_scaled[column] = 0\n",
    "for i in range(folds):\n",
    "    print ('Fold {}'.format(i))\n",
    "    indices = sorted(random.sample(list(full_index), entries))\n",
    "    fold_indices.append(indices)\n",
    "    full_index = full_index.drop(indices)\n",
    "\n",
    "    full_index_keep = np.zeros((full_length), dtype=np.bool)\n",
    "    full_index_throw = np.ones((full_length), dtype=np.bool)\n",
    "    for j in fold_indices[i]:\n",
    "        full_index_keep[j] = True\n",
    "        full_index_throw[j] = False\n",
    "    fold_indices_throw.append(full_index_throw)\n",
    "    fold_indices_keep.append(full_index_keep)\n",
    "    \n",
    "    #k_means = KMeans(n_clusters = 5)\n",
    "    #k_means.fit(train_X_scaled[knn_features][full_index_throw], train_y[full_index_throw])\n",
    "    #k_means_indices = k_means.predict(train_X_scaled[knn_features][full_index_keep])\n",
    "    #train_X_scaled.loc[indices, 'k_means'] = k_means_indices\n",
    "    \n",
    "    #for n_neighbors in [2, 4, 8, 16, 64, 256, 1024]:\n",
    "    #    knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric='manhattan') # mahalanobis\n",
    "    #    knn.fit(train_X_scaled[knn_features][full_index_throw], train_y[full_index_throw])\n",
    "    #    knn_pred = knn.predict(train_X_scaled[knn_features][full_index_keep])\n",
    "    #    train_X_scaled.loc[indices, 'knn_{}'.format(n_neighbors)] = knn_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_scaled.to_json('data/train_scaled_with_leaky_and_added_features_cv.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20: loss = 0.6258024382212498, val = 0.6258474560374475\n",
      "epoch 40: loss = 0.6015593465094552, val = 0.6047516582676533\n",
      "epoch 60: loss = 0.5855522775576594, val = 0.5948427323391458\n",
      "epoch 80: loss = 0.5746987507648224, val = 0.588205450501738\n",
      "epoch 100: loss = 0.5684100572236606, val = 0.5846041348402148\n",
      "epoch 120: loss = 0.5586724175190484, val = 0.5812287285795937\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_X_cv = pd.read_json('data/train_with_leaky_cv.json')\n",
    "train_X_scaled_cv = pd.DataFrame(StandardScaler().fit_transform(train_X_cv), columns=train_X_cv.columns)\n",
    "\n",
    "ntrain = int(len(train_X_cv) * 0.75)\n",
    "\n",
    "xtr = train_X_scaled_cv[:ntrain]\n",
    "ytr = train_y[:ntrain].values\n",
    "ytr_one_hot = np.eye(3)[ytr]\n",
    "xte = train_X_scaled_cv[ntrain:]\n",
    "yte = train_y[ntrain:].values\n",
    "yte_one_hot = np.eye(3)[yte]\n",
    "\n",
    "def create_model(xtr):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(500, input_dim=xtr.shape[1], init = 'he_normal', activation='sigmoid'))\n",
    "    #model.add(Dense(64, input_dim=64, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(PReLU())\n",
    "\n",
    "    model.add(Dense(100, init = 'he_normal', activation='sigmoid'))\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(PReLU())\n",
    "\n",
    "    model.add(Dense(3, init = 'he_normal', activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "    return model\n",
    "\n",
    "filepath = 'data/nn_weights'\n",
    "\n",
    "logger = NBatchLogger()\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=40, verbose=0)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "model = create_model(xtr)\n",
    "model.fit(xtr.values, ytr_one_hot, nb_epoch = 1200, batch_size=1000, verbose = 0, validation_data=[xte.values, yte_one_hot], callbacks=[logger, early_stop, checkpoint])\n",
    "model = create_model(xtr)\n",
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = pd.read_json('data/train_leaky.json')\n",
    "train_X_scaled = pd.DataFrame(StandardScaler().fit_transform(train_X), columns=train_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrain = int(len(train_X) * 0.75)\n",
    "\n",
    "xtr = train_X_scaled[:ntrain]\n",
    "ytr = train_y[:ntrain].values\n",
    "ytr_one_hot = np.eye(3)[ytr]\n",
    "xte = train_X_scaled[ntrain:]\n",
    "yte = train_y[ntrain:].values\n",
    "yte_one_hot = np.eye(3)[yte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new_vects = ['tf_idf_' + x[7:] for x in sparse_cols]\n",
    "#sparse = processed_train[sparse_cols]\n",
    "#tf_idfs = TfidfTransformer().fit_transform(sparse)\n",
    "#tf_idfs_df = pd.DataFrame(tf_idfs.toarray(), columns=new_vects)\n",
    "#processed_train = processed_train.join(tf_idfs_df, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(xtr):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(500, input_dim=xtr.shape[1], init = 'he_normal', activation='sigmoid'))\n",
    "    #model.add(Dense(64, input_dim=64, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(PReLU())\n",
    "\n",
    "    model.add(Dense(100, init = 'he_normal', activation='sigmoid'))\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(PReLU())\n",
    "\n",
    "    model.add(Dense(3, init = 'he_normal', activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20: loss = 0.6089174983444364, val = 0.6067765715308119\n",
      "epoch 40: loss = 0.5853107286090885, val = 0.5850755318499284\n",
      "epoch 60: loss = 0.5705643378497265, val = 0.5771795387089513\n",
      "epoch 80: loss = 0.5579046620334922, val = 0.5707317711294789\n",
      "epoch 100: loss = 0.5534765775343996, val = 0.5669029398624583\n",
      "epoch 120: loss = 0.546212861740448, val = 0.5643390875172627\n",
      "epoch 140: loss = 0.5400573863694326, val = 0.5620383667605761\n",
      "epoch 160: loss = 0.5329683061201813, val = 0.5618096001435378\n",
      "epoch 180: loss = 0.5292632116061488, val = 0.5612778273195361\n",
      "epoch 200: loss = 0.5248733873195198, val = 0.5598880919534396\n",
      "epoch 220: loss = 0.5186089570991107, val = 0.5604438877101848\n",
      "epoch 240: loss = 0.5180035672343942, val = 0.5601027862993587\n",
      "epoch 260: loss = 0.5172649729823205, val = 0.5597893895989281\n",
      "epoch 280: loss = 0.5114319491774181, val = 0.5599595837972601\n",
      "CPU times: user 22min 33s, sys: 2min 25s, total: 24min 59s\n",
      "Wall time: 18min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filepath = 'data/nn_weights'\n",
    "\n",
    "logger = NBatchLogger()\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=40, verbose=0)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "model = create_model(xtr)\n",
    "model.fit(xtr.values, ytr_one_hot, nb_epoch = 1200, batch_size=1000, verbose = 0, validation_data=[xte.values, yte_one_hot], callbacks=[logger, early_stop, checkpoint])\n",
    "model = create_model(xtr)\n",
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model(xtr)\n",
    "model.load_weights('data/nn_weights_not_cv')\n",
    "nn_predictions = model.predict(xte.values)\n",
    "print (log_loss(yte_one_hot, nn_predictions))\n",
    "test_X = pd.read_json('data/test_leaky.json')\n",
    "test_X = pd.DataFrame(scaler.transform(test_X), columns=columns)\n",
    "listing_id = pd.read_json('data/test_ids.json').valuespreds = model.predict(test_X.values)\n",
    "preds = pd.DataFrame(preds)\n",
    "preds.columns = ['high', 'medium', 'low']\n",
    "preds['listing_id'] = listing_id\n",
    "preds.to_csv('data/my_preds_not_cv.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model(xtr)\n",
    "model.load_weights('data/nn_weights')\n",
    "nn_predictions = model.predict(xte.values)\n",
    "print (log_loss(yte_one_hot, nn_predictions))\n",
    "test_X = pd.read_json('data/test_leaky.json')\n",
    "test_X = pd.DataFrame(scaler.transform(test_X), columns=columns)\n",
    "listing_id = pd.read_json('data/test_ids.json').valuespreds = model.predict(test_X.values)\n",
    "preds = pd.DataFrame(preds)\n",
    "preds.columns = ['high', 'medium', 'low']\n",
    "preds['listing_id'] = listing_id\n",
    "preds.to_csv('data/my_preds_cv.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60016923349431917"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_predictions = model.predict(xte.values)\n",
    "log_loss(yte_one_hot, nn_predictions)\n",
    "#0.56359369155527683 before adding pca, knn, mkeans to predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Lasso Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726779056573\n",
      "9.43671707143\n"
     ]
    }
   ],
   "source": [
    "clf = RidgeClassifier()\n",
    "clf.fit(xtr, ytr)\n",
    "print (clf.score(xte, yte))\n",
    "predictions = clf.predict(xte)\n",
    "print (log_loss(yte_one_hot, np.eye(3)[predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "las = Lasso()\n",
    "enet = ElasticNet()\n",
    "las.fit(xte, yte)\n",
    "enet.fit(xte, yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "10.478087214\n"
     ]
    }
   ],
   "source": [
    "print (las.score(xte, yte))\n",
    "predictions = las.predict(xte)\n",
    "print (log_loss(yte_one_hot, np.eye(3)[np.round(predictions).astype(int)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "10.478087214\n"
     ]
    }
   ],
   "source": [
    "print (enet.score(xte, yte))\n",
    "predictions = enet.predict(xte)\n",
    "print (log_loss(yte_one_hot, np.eye(3)[np.round(predictions).astype(int)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for coef, column in zip(enet.coef_, train_X.columns):\n",
    "    if coef != 0:\n",
    "        print (column, \": \", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for coef, column in zip(las.coef_, train_X.columns):\n",
    "    if coef != 0:\n",
    "        print (column, \": \", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5843451108603969 (test loss 0.3912286072900344)\n"
     ]
    }
   ],
   "source": [
    "test = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "\n",
    "test.fit(xtr, ytr)\n",
    "            \n",
    "predictions = test.predict_proba(xtr)\n",
    "et_predictions = test.predict_proba(xte)\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, et_predictions), log_loss(ytr_one_hot, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5841850290743527 (test loss 0.3945679429419886)\n"
     ]
    }
   ],
   "source": [
    "test = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "\n",
    "test.fit(xtr, ytr)\n",
    "            \n",
    "predictions = test.predict_proba(xtr)\n",
    "et_ent_predictions = test.predict_proba(xte)\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, et_ent_predictions), log_loss(ytr_one_hot, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5693372215730407 (test loss 0.3780482702094549)\n"
     ]
    }
   ],
   "source": [
    "test = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "\n",
    "test.fit(xtr, ytr)\n",
    "            \n",
    "predictions = test.predict_proba(xtr)\n",
    "rf_predictions = test.predict_proba(xte)\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, rf_predictions), log_loss(ytr_one_hot, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5668175898671493 (test loss 0.3773447326368733)\n"
     ]
    }
   ],
   "source": [
    "test = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "\n",
    "test.fit(xtr, ytr)\n",
    "            \n",
    "predictions = test.predict_proba(xtr)\n",
    "rf_ent_predictions = test.predict_proba(xte)\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, rf_ent_predictions), log_loss(ytr_one_hot, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56688904952650898"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mixed = (0.01 * et_predictions + 0.01 * et_ent_predictions + 0.49 * rf_predictions + 0.49 * rf_ent_predictions)\n",
    "log_loss(yte_one_hot, pred_mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.076257047822155 (test loss 0.23129250627049086)\n",
      "CPU times: user 3min 15s, sys: 1.48 s, total: 3min 16s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=2)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions = test.predict_proba(xte[non_sparse_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.997161309463761 (test loss 0.4189513500335195)\n",
      "CPU times: user 3min 36s, sys: 1.02 s, total: 3min 37s\n",
      "Wall time: 3min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=5)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions_2 = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions_2 = test.predict_proba(xte[non_sparse_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5474359459916005 (test loss 0.4994428601539661)\n",
      "CPU times: user 3min 48s, sys: 839 ms, total: 3min 49s\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=10)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions_3 = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions_3 = test.predict_proba(xte[non_sparse_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 9s, sys: 1.36 s, total: 4min 11s\n",
      "Wall time: 4min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=20)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions_4 = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions_4 = test.predict_proba(xte[non_sparse_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 18s, sys: 1.2 s, total: 4min 19s\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=50)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions_5 = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions_5 = test.predict_proba(xte[non_sparse_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 17s, sys: 1.06 s, total: 4min 18s\n",
      "Wall time: 4min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=50)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions_45 = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions_45 = test.predict_proba(xte[non_sparse_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 26s, sys: 1.07 s, total: 4min 27s\n",
      "Wall time: 4min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=100)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions_5 = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions_5 = test.predict_proba(xte[non_sparse_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://mlwave.com/kaggle-ensembling-guide/\n",
    "https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14295\n",
    "https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6344317626130038 (test loss 0.6195293255389913)\n",
      "CPU times: user 4min 34s, sys: 1.19 s, total: 4min 35s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=200)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions_6 = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions_6 = test.predict_proba(xte[non_sparse_cols])\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, rf_ent_predictions_6), log_loss(ytr_one_hot, predictions_6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7014805929988618 (test loss 0.5857152625149388)\n"
     ]
    }
   ],
   "source": [
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, rf_ent_predictions_45), log_loss(ytr_one_hot, predictions_45)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6436552097433358 (test loss 0.6045059334208441)\n"
     ]
    }
   ],
   "source": [
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, rf_ent_predictions_5), log_loss(ytr_one_hot, predictions_5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feat, importance in zip(processed_train[cols].columns, clf.feature_importances_):\n",
    "    if not np.isnan(importance) and importance > 0:\n",
    "        print (feat, \": \", importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn:  0.563593691555\n",
      "ab:  0.566889049527\n",
      "combo:  0.55827286106\n"
     ]
    }
   ],
   "source": [
    "print (\"nn: \", log_loss(yte_one_hot, nn_predictions))\n",
    "print (\"ab: \", log_loss(yte_one_hot, pred_mixed))\n",
    "frac_nn = 0.2\n",
    "averaged_preds = (nn_predictions * frac_nn) + (pred_mixed * (1 - frac_nn))\n",
    "print (\"combo: \", log_loss(yte_one_hot, averaged_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = pd.read_json('data/train_leaky.json')\n",
    "train_y = pd.read_json('data/train_interest.json')[0]\n",
    "scaler = StandardScaler().fit(train_X)\n",
    "train_X = pd.DataFrame(scaler.transform(train_X), columns=columns)\n",
    "test_X = pd.read_json('data/test_leaky.json')\n",
    "test_X = pd.DataFrame(scaler.transform(test_X), columns=columns)\n",
    "listing_id = pd.read_json('data/test_ids.json').values\n",
    "preds = pd.read_csv('data/my_best_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = pd.read_csv('data/my_best_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_clf = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='gini',\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "et_clf.fit(train_X, train_y)          \n",
    "et_predictions = et_clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_clf_ent = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "et_clf_ent.fit(train_X, train_y)          \n",
    "et_ent_predictions = et_clf_ent.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='gini',\n",
    "    min_samples_split=20)\n",
    "rf_clf.fit(train_X, train_y)          \n",
    "rf_predictions = rf_clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_clf_ent = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    min_samples_split=20)\n",
    "rf_clf_ent.fit(train_X, train_y)          \n",
    "rf_ent_predictions = rf_clf_ent.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_predictions = model.predict(test_X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_predictions = preds[['high', 'medium', 'low']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_preds = (0.8 * xgb_predictions) + (0.15 * nn_predictions) + (0.05 * et_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_preds_df = pd.DataFrame(new_preds, columns=['high', 'medium', 'low'])\n",
    "new_preds_df['listing_id'] = preds['listing_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>medium</th>\n",
       "      <th>low</th>\n",
       "      <th>listing_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.062057</td>\n",
       "      <td>0.562695</td>\n",
       "      <td>0.375248</td>\n",
       "      <td>7142618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.072976</td>\n",
       "      <td>0.033289</td>\n",
       "      <td>0.893735</td>\n",
       "      <td>7210040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.995199</td>\n",
       "      <td>6832604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130962</td>\n",
       "      <td>0.298393</td>\n",
       "      <td>0.570644</td>\n",
       "      <td>6830595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.989359</td>\n",
       "      <td>6843709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       high    medium       low  listing_id\n",
       "0  0.062057  0.562695  0.375248     7142618\n",
       "1  0.072976  0.033289  0.893735     7210040\n",
       "2  0.000501  0.004300  0.995199     6832604\n",
       "3  0.130962  0.298393  0.570644     6830595\n",
       "4  0.002368  0.008273  0.989359     6843709"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>medium</th>\n",
       "      <th>low</th>\n",
       "      <th>listing_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.057388</td>\n",
       "      <td>0.528401</td>\n",
       "      <td>0.414212</td>\n",
       "      <td>7142618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>0.031273</td>\n",
       "      <td>0.908920</td>\n",
       "      <td>7210040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.005621</td>\n",
       "      <td>0.993565</td>\n",
       "      <td>6832604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129824</td>\n",
       "      <td>0.305827</td>\n",
       "      <td>0.564349</td>\n",
       "      <td>6830595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.011676</td>\n",
       "      <td>0.985567</td>\n",
       "      <td>6843709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       high    medium       low  listing_id\n",
       "0  0.057388  0.528401  0.414212     7142618\n",
       "1  0.059807  0.031273  0.908920     7210040\n",
       "2  0.000814  0.005621  0.993565     6832604\n",
       "3  0.129824  0.305827  0.564349     6830595\n",
       "4  0.002757  0.011676  0.985567     6843709"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_preds_df.to_csv('data/combined_preds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
