{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgzuke/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.metrics import log_loss\n",
    "import string\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from scipy.stats import boxcox\n",
    "from scipy import stats\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file = 'data/train.json'\n",
    "train = pd.read_json(train_file)\n",
    "test_file = 'data/test.json'\n",
    "test = pd.read_json(test_file)\n",
    "\n",
    "listing_id = test.listing_id.values\n",
    "\n",
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "train['interest_level'] = train['interest_level'].apply(lambda x: y_map[x])\n",
    "y_train = train.interest_level.reset_index(drop=True)\n",
    "\n",
    "train = train.drop(['listing_id', 'interest_level'], axis=1)\n",
    "test = test.drop('listing_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fpreproc_safe(train, test):\n",
    "    ntrain = train.shape[0]\n",
    "    train_test = pd.concat((train, test), axis=0).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # convert date to be more useable\n",
    "    train_test['Date'] = pd.to_datetime(train_test['created'])\n",
    "    # year is all the same\n",
    "    #train_test['Year'] = train_test['Date'].dt.year\n",
    "    train_test['Month'] = train_test['Date'].dt.month\n",
    "    train_test['Day'] = train_test['Date'].dt.day\n",
    "    train_test['Wday'] = train_test['Date'].dt.dayofweek\n",
    "    train_test['Yday'] = train_test['Date'].dt.dayofyear\n",
    "    train_test['hour'] = train_test['Date'].dt.hour\n",
    "    train_test = train_test.drop(['Date', 'created'], axis=1)\n",
    "\n",
    "\n",
    "    # check if this building id/manager id and add worths is 0\n",
    "    train_test['Zero_building_id'] = train_test['building_id'].apply(lambda x: 1 if x == '0' else 0)\n",
    "    train_test['Zero_manager_id'] = train_test['manager_id'].apply(lambda x: 1 if x == '0' else 0)\n",
    "    \n",
    "\n",
    "    # mess with description meta data\n",
    "    train_test['desc'] = train_test['description']\n",
    "    train_test['desc'] = train_test['desc'].apply(lambda x: x.replace('<p><a  website_redacted ', ''))\n",
    "    train_test['desc'] = train_test['desc'].apply(lambda x: x.replace('!<br /><br />', ''))\n",
    "\n",
    "    string.punctuation.__add__('!!')\n",
    "    string.punctuation.__add__('(')\n",
    "    string.punctuation.__add__(')')\n",
    "    remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "    train_test['desc'] = train_test['desc'].apply(lambda x: x.translate(remove_punct_map))\n",
    "    train_test['desc_letters_count'] = train_test['description'].apply(lambda x: len(x.strip()))\n",
    "    train_test['desc_words_count'] = train_test['desc'].apply(lambda x: 0 if len(x.strip()) == 0 else len(x.split(' ')))\n",
    "    train_test['desc_words_length'] = (train_test['desc_letters_count'] / train_test['desc_words_count']).apply(lambda x: 0 if math.isnan(x) or math.isinf(x) else x)\n",
    "\n",
    "    train_test.drop(['description', 'desc'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # address\n",
    "    train_test['address1'] = train_test['display_address']\n",
    "    train_test['address1'] = train_test['address1'].apply(lambda x: x.lower())\n",
    "\n",
    "    address_map = {\n",
    "        'w': 'west',\n",
    "        'st.': 'street',\n",
    "        'ave': 'avenue',\n",
    "        'st': 'street',\n",
    "        'e': 'east',\n",
    "        'n': 'north',\n",
    "        's': 'south'\n",
    "    }\n",
    "\n",
    "    def address_map_func(s):\n",
    "        s = s.split(' ')\n",
    "        out = []\n",
    "        for x in s:\n",
    "            if x in address_map:\n",
    "                out.append(address_map[x])\n",
    "            else:\n",
    "                out.append(x)\n",
    "        return ' '.join(out)\n",
    "\n",
    "\n",
    "    train_test['address1'] = train_test['address1'].apply(lambda x: x.translate(remove_punct_map))\n",
    "    train_test['address1'] = train_test['address1'].apply(lambda x: address_map_func(x))\n",
    "\n",
    "    new_cols = ['street', 'avenue', 'east', 'west', 'north', 'south']\n",
    "    for col in new_cols:\n",
    "        train_test[col] = train_test['address1'].apply(lambda x: 1 if col in x else 0)\n",
    "\n",
    "    train_test['other_address'] = train_test[new_cols].apply(lambda x: 1 if x.sum() == 0 else 0, axis=1)\n",
    "    train_test.drop(['display_address', 'street_address'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # features (better not to lower)\n",
    "    train_test['features_count'] = train_test['features'].apply(lambda x: len(x))\n",
    "    train_test['features2'] = train_test['features']\n",
    "    train_test['features2'] = train_test['features2'].apply(lambda x: ' '.join([''.join(i.replace('_',' ').replace('-',' ').split(' ')) for i in x]))\n",
    "\n",
    "    c_vect = CountVectorizer(stop_words='english', max_features=200, ngram_range=(1, 1))\n",
    "    c_vect.fit(train_test['features2'])\n",
    "\n",
    "    c_vect_sparse_1 = c_vect.transform(train_test['features2'])\n",
    "    c_vect_sparse1_cols = c_vect.get_feature_names()\n",
    "    train_test.drop(['features', 'features2'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # photos\n",
    "    train_test['photos_count'] = train_test['photos'].apply(lambda x: len(x))\n",
    "    train_test.drop(['photos'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # convert ['building_id', 'manager_id', 'address1'] to enumerated labels\n",
    "    categoricals = [x for x in train_test.columns if train_test[x].dtype == 'object']\n",
    "    for feat in categoricals:\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(train_test[feat].values))\n",
    "        train_test[feat] = lbl.transform(list(train_test[feat].values))\n",
    "\n",
    "\n",
    "    # convert bed/bath number to enumeration\n",
    "    train_test['bathrooms_cat'] = train_test['bathrooms'].apply(lambda x: str(x))\n",
    "    train_test['bathrooms_cat'], labels = pd.factorize(train_test['bathrooms_cat'].values, sort=True)\n",
    "    train_test.drop('bathrooms', axis=1, inplace=True)\n",
    "    train_test['bedroom_cat'], labels = pd.factorize(train_test['bedrooms'].values, sort=True)\n",
    "    train_test.drop('bedrooms', axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    # transform managers and building ids\n",
    "    train_test['manager_id'] = train_test['manager_id'].apply(lambda x: str(x))\n",
    "    train_test['manager_id'], labels = pd.factorize(train_test['manager_id'].values, sort=True)\n",
    "    train_test['building_id'] = train_test['building_id'].apply(lambda x: str(x))\n",
    "    train_test['building_id'], labels = pd.factorize(train_test['building_id'].values, sort=True)\n",
    "    \n",
    "\n",
    "    # transform price\n",
    "    bc_price, tmp = boxcox(train_test.price)\n",
    "    train_test['bc_price'] = bc_price\n",
    "\n",
    "\n",
    "    # add sparse\n",
    "    train_test_cv1_sparse = sparse.hstack((train_test, c_vect_sparse_1)).tocsr()\n",
    "    x_train = train_test_cv1_sparse[:ntrain, :]\n",
    "    x_test = train_test_cv1_sparse[ntrain:, :]\n",
    "\n",
    "\n",
    "    # add feature names\n",
    "    features = list(train_test.columns) + ['sparse_' + vect_name for vect_name in c_vect_sparse1_cols]\n",
    "    dtrain_data = pd.DataFrame(np.array(x_train.todense()), columns=features)\n",
    "    dtest_data = pd.DataFrame(np.array(x_test.todense()), columns=features)\n",
    "    return dtrain_data, dtest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fpreproc_leaky(dtrain, dtest, param):\n",
    "    train, test = dtrain.data, dtest.data\n",
    "    ntrain = train.shape[0]\n",
    "    train_test = pd.concat((train, test), axis=0).reset_index(drop=True)\n",
    "    y_train = dtrain.labels\n",
    "    \n",
    "    # add manager worths\n",
    "    manager_ids = np.unique(train.manager_id)\n",
    "    manager_worths = {}\n",
    "    for manager_id in manager_ids:\n",
    "        interests = y_train[train.manager_id == manager_id].apply(lambda x: 2 - x)\n",
    "        if len(interests) > 5:\n",
    "            manager_worths[manager_id] = sum(interests) / len(interests)\n",
    "    average_interest = y_train.apply(lambda x: 2 - x)\n",
    "    average_manager_worth = 0 #sum(average_interest) / len(average_interest)\n",
    "    train_test['manager_worths'] = train_test['manager_id'].apply(lambda x: manager_worths[x] if x in manager_worths else average_manager_worth)\n",
    "\n",
    "    \n",
    "    # add building worths\n",
    "    building_ids = np.unique(train.building_id)\n",
    "    building_worths = {}\n",
    "    for building_id in building_ids:\n",
    "        interests = y_train[train.building_id == building_id].apply(lambda x: 2 - x)\n",
    "        if len(interests) > 5:\n",
    "            building_worths[building_id] = sum(interests) / len(interests)\n",
    "    average_interest = y_train.apply(lambda x: 2 - x)\n",
    "    average_building_worth = 0 #sum(average_interest) / len(average_interest)\n",
    "    train_test['building_worths'] = train_test['building_id'].apply(lambda x: building_worths[x] if x in building_worths else average_building_worth)\n",
    "\n",
    "    \n",
    "    # add price by area\n",
    "    lat_long_price = train_test[['latitude', 'longitude', 'price']]\n",
    "    remove_outliers = (np.abs(stats.zscore(lat_long_price)) < 0.15).all(axis=1)\n",
    "    lat_long_price = lat_long_price[remove_outliers]\n",
    "    lat_max, lat_min = max(lat_long_price.latitude), min(lat_long_price.latitude)\n",
    "    long_max, long_min = max(lat_long_price.longitude), min(lat_long_price.longitude)\n",
    "    lat_scale, long_scale = lat_max - lat_min, long_max - long_min\n",
    "    costs = np.zeros((100,100))\n",
    "    num_listings = np.zeros((100,100))\n",
    "    for lat, long, price in lat_long_price.values:\n",
    "        scaled_lat, scaled_long = int((lat - lat_min) * 99 / lat_scale), int((long - long_min) * 99 / long_scale)\n",
    "        costs[scaled_lat][scaled_long] += price\n",
    "        num_listings[scaled_lat][scaled_long] += 1\n",
    "\n",
    "    price_by_area = []\n",
    "    for lat, long, price in train_test[['latitude', 'longitude', 'price']].values:\n",
    "        scaled_lat, scaled_long = int((lat - lat_min) * 99 / lat_scale), int((long - long_min) * 99 / long_scale)\n",
    "        if scaled_lat < 0 or scaled_lat >= 100 or scaled_long < 0 or scaled_long >= 100:\n",
    "            price_by_area.append(0)\n",
    "        elif num_listings[scaled_lat][scaled_long] > 8:\n",
    "            price_by_area.append(price - (costs[scaled_lat][scaled_long] / num_listings[scaled_lat][scaled_long]))\n",
    "        else:\n",
    "            cost = 0\n",
    "            num = 0\n",
    "            for i in range(scaled_lat - 1, scaled_lat + 2):\n",
    "                for j in range(scaled_long - 1, scaled_long + 2):\n",
    "                    if i > 0 and i < 100 and j >= 0 and j < 100:\n",
    "                        cost += costs[i][j]\n",
    "                        num += num_listings[i][j]\n",
    "            if num > 8:\n",
    "                price_by_area.append(price - (cost / num))\n",
    "            else:\n",
    "                price_by_area.append(0)\n",
    "\n",
    "    train_test['price_by_area'] = price_by_area\n",
    "    \n",
    "    \n",
    "    # try adding real - predicted price\n",
    "    # Try to predict price for a listing and add real_price - expected_price as a feature\n",
    "    # train on [exclusive, byowner, nofee] and #bed, #bath, display address\n",
    "    features_to_use = ['bathrooms_cat', 'bedroom_cat', 'street', 'avenue', 'east', 'west', 'north', 'south', 'address1', 'other_address', 'building_worths', 'manager_worths']\n",
    "    feature_to_predict = 'price'\n",
    "    params = {\n",
    "        'objective': 'reg:linear',\n",
    "        'booster':'gblinear',\n",
    "        'lambda': 0,\n",
    "        'lambda_bias' : 0,\n",
    "        'alpha': 0.2\n",
    "    }\n",
    "    prices = train_test[feature_to_predict]\n",
    "    remove_outliers = np.abs(prices-prices.mean())<=(3*prices.std())\n",
    "    dtrain = xgb.DMatrix(data=train_test[remove_outliers][features_to_use], label=train_test[remove_outliers][feature_to_predict])\n",
    "\n",
    "    bst = xgb.cv(params, dtrain, 10000, 4, early_stopping_rounds=50, verbose_eval=200)\n",
    "    best_rounds = np.argmin(bst['test-rmse-mean'])\n",
    "    print (bst['test-rmse-mean'][best_rounds])\n",
    "    bst = xgb.train(params, dtrain, best_rounds)\n",
    "    dtrain = xgb.DMatrix(data=train_test[features_to_use])\n",
    "    expected_price = bst.predict(dtrain)\n",
    "    train_test['real_minus_expected_price'] = train_test[feature_to_predict] - expected_price\n",
    "    train_test['real_over_expected_price'] = train_test[feature_to_predict] / expected_price\n",
    "    \n",
    "    \n",
    "    # remove extra\n",
    "    train_test.drop('price', axis=1, inplace=True)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(data=train_test[:ntrain], label=y_train)\n",
    "    dtest = xgb.DMatrix(data=train_test[ntrain:], label=dtest.labels)\n",
    "    return dtrain, dtest, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FakeDMatrix:\n",
    "    def __init__(self, data, labels=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.num = len(data)\n",
    "\n",
    "    def num_row(self):\n",
    "       return self.num\n",
    "\n",
    "    def slice(self, rindex):\n",
    "        indices = np.zeros(self.num, dtype=np.bool)\n",
    "        for index in rindex:\n",
    "            indices[index] = True\n",
    "        return FakeDMatrix(data=self.data[indices], labels=self.labels[indices])\n",
    "    \n",
    "SEED = 777\n",
    "NFOLDS = 5\n",
    "\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.8,\n",
    "    'subsample':.8,\n",
    "    'seed':0,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "dtrain_data, dtest_data = fpreproc_safe(train, test)\n",
    "dtrain = FakeDMatrix(data=dtrain_data, labels=y_train)\n",
    "dtest = FakeDMatrix(data=dtest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2028.32+126.79\ttest-rmse:2028.72+98.4538\n",
      "[200]\ttrain-rmse:1643.14+18.1906\ttest-rmse:1643.01+54.2032\n",
      "1643.00567625\n",
      "[0]\ttrain-rmse:2036.09+93.4448\ttest-rmse:2038.92+79.9489\n",
      "1644.18063375\n",
      "[0]\ttrain-rmse:1977.34+200.909\ttest-rmse:1984.64+162.267\n",
      "[200]\ttrain-rmse:1644.02+30.0261\ttest-rmse:1642.68+87.4003\n",
      "1642.67880225\n",
      "[0]\ttrain-rmse:1958.08+91.497\ttest-rmse:1957.9+74.6673\n",
      "1643.44158925\n",
      "[0]\ttrain-rmse:1935.69+74.271\ttest-rmse:1929.68+77.2772\n",
      "1642.98703025\n",
      "[0]\ttrain-mlogloss:1.0917+2.33838e-05\ttest-mlogloss:1.09194+6.62468e-05\n",
      "[20]\ttrain-mlogloss:0.97181+0.000262769\ttest-mlogloss:0.977435+0.000729137\n",
      "[40]\ttrain-mlogloss:0.882608+0.000580355\ttest-mlogloss:0.893044+0.000855938\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200, fpreproc=fpreproc_leaky)\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "print (bst['test-mlogloss-mean'][best_rounds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain_final, dtest_final = fpreproc_leaky(dtrain, dtest)\n",
    "bst = xgb.train(params, dtrain_final, best_rounds)\n",
    "preds = bst.predict(dtest_final)\n",
    "\n",
    "preds = pd.DataFrame(preds)\n",
    "cols = ['high', 'medium', 'low']\n",
    "preds.columns = cols\n",
    "preds['listing_id'] = listing_id\n",
    "preds.to_csv('my_preds.csv', index=None)\n",
    "\n",
    "importance = bst.get_fscore()\n",
    "feature_importance = [(feature, (importance['f{}'.format(i)]) if 'f{}'.format(i) in importance else 0) for i, feature in enumerate(features)]\n",
    "sorted(feature_importance, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.8 s, sys: 266 ms, total: 20.1 s\n",
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "manager_ids = np.unique(train.manager_id)\n",
    "manager_worths = {}\n",
    "for manager_id in manager_ids:\n",
    "    interests = y_train[train.manager_id == manager_id].apply(lambda x: 2 - x)\n",
    "    if len(interests) > 5:\n",
    "        manager_worths[manager_id] = sum(interests) / len(interests)\n",
    "average_interest = y_train.apply(lambda x: 2 - x)\n",
    "average_manager_worth = 0 #sum(average_interest) / len(average_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.2 s, sys: 515 ms, total: 39.7 s\n",
      "Wall time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "building_ids = np.unique(train.building_id)\n",
    "building_worths = {}\n",
    "for building_id in building_ids:\n",
    "    interests = y_train[train.building_id == building_id].apply(lambda x: 2 - x)\n",
    "    if len(interests) > 5:\n",
    "        building_worths[building_id] = sum(interests) / len(interests)\n",
    "average_interest = y_train.apply(lambda x: 2 - x)\n",
    "average_building_worth = 0 #sum(average_interest) / len(average_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 447 ms, sys: 3.3 ms, total: 450 ms\n",
      "Wall time: 451 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lat_long_price = train_test[['latitude', 'longitude', 'price']]\n",
    "remove_outliers = (np.abs(stats.zscore(lat_long_price)) < 0.15).all(axis=1)\n",
    "lat_long_price = lat_long_price[remove_outliers]\n",
    "lat_max, lat_min = max(lat_long_price.latitude), min(lat_long_price.latitude)\n",
    "long_max, long_min = max(lat_long_price.longitude), min(lat_long_price.longitude)\n",
    "lat_scale, long_scale = lat_max - lat_min, long_max - long_min\n",
    "costs = np.zeros((100,100))\n",
    "num_listings = np.zeros((100,100))\n",
    "for lat, long, price in lat_long_price.values:\n",
    "    scaled_lat, scaled_long = int((lat - lat_min) * 99 / lat_scale), int((long - long_min) * 99 / long_scale)\n",
    "    costs[scaled_lat][scaled_long] += price\n",
    "    num_listings[scaled_lat][scaled_long] += 1\n",
    "\n",
    "price_by_area = []\n",
    "for lat, long, price in train_test[['latitude', 'longitude', 'price']].values:\n",
    "    scaled_lat, scaled_long = int((lat - lat_min) * 99 / lat_scale), int((long - long_min) * 99 / long_scale)\n",
    "    if scaled_lat < 0 or scaled_lat >= 100 or scaled_long < 0 or scaled_long >= 100:\n",
    "        price_by_area.append(0)\n",
    "    elif num_listings[scaled_lat][scaled_long] > 8:\n",
    "        price_by_area.append(price - (costs[scaled_lat][scaled_long] / num_listings[scaled_lat][scaled_long]))\n",
    "    else:\n",
    "        cost = 0\n",
    "        num = 0\n",
    "        for i in range(scaled_lat - 1, scaled_lat + 2):\n",
    "            for j in range(scaled_long - 1, scaled_long + 2):\n",
    "                if i > 0 and i < 100 and j >= 0 and j < 100:\n",
    "                    cost += costs[i][j]\n",
    "                    num += num_listings[i][j]\n",
    "        if num > 8:\n",
    "            price_by_area.append(price - (cost / num))\n",
    "        else:\n",
    "            price_by_area.append(0)\n",
    "\n",
    "train_test['price_by_area'] = price_by_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2005.81+18.7791\ttest-rmse:2005.4+62.831\n",
      "[200]\ttrain-rmse:1638.46+20.0772\ttest-rmse:1638.04+60.2721\n",
      "1638.0352785\n",
      "CPU times: user 17.3 s, sys: 1.25 s, total: 18.6 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# convert date to be more useable\n",
    "train_test['Date'] = pd.to_datetime(train_test['created'])\n",
    "# year is all the same\n",
    "#train_test['Year'] = train_test['Date'].dt.year\n",
    "train_test['Month'] = train_test['Date'].dt.month\n",
    "train_test['Day'] = train_test['Date'].dt.day\n",
    "train_test['Wday'] = train_test['Date'].dt.dayofweek\n",
    "train_test['Yday'] = train_test['Date'].dt.dayofyear\n",
    "train_test['hour'] = train_test['Date'].dt.hour\n",
    "train_test = train_test.drop(['Date', 'created'], axis=1)\n",
    "\n",
    "\n",
    "# check if this building id/manager id and add worths is 0\n",
    "train_test['Zero_building_id'] = train_test['building_id'].apply(lambda x: 1 if x == '0' else 0)\n",
    "train_test['Zero_manager_id'] = train_test['manager_id'].apply(lambda x: 1 if x == '0' else 0)\n",
    "train_test['manager_worths'] = train_test['manager_id'].apply(lambda x: manager_worths[x] if x in manager_worths else average_manager_worth)\n",
    "train_test['building_worths'] = train_test['building_id'].apply(lambda x: building_worths[x] if x in building_worths else average_building_worth)\n",
    "\n",
    "\n",
    "# mess with description meta data\n",
    "train_test['desc'] = train_test['description']\n",
    "train_test['desc'] = train_test['desc'].apply(lambda x: x.replace('<p><a  website_redacted ', ''))\n",
    "train_test['desc'] = train_test['desc'].apply(lambda x: x.replace('!<br /><br />', ''))\n",
    "\n",
    "string.punctuation.__add__('!!')\n",
    "string.punctuation.__add__('(')\n",
    "string.punctuation.__add__(')')\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "train_test['desc'] = train_test['desc'].apply(lambda x: x.translate(remove_punct_map))\n",
    "train_test['desc_letters_count'] = train_test['description'].apply(lambda x: len(x.strip()))\n",
    "train_test['desc_words_count'] = train_test['desc'].apply(lambda x: 0 if len(x.strip()) == 0 else len(x.split(' ')))\n",
    "train_test['desc_words_length'] = (train_test['desc_letters_count'] / train_test['desc_words_count']).apply(lambda x: 0 if math.isnan(x) or math.isinf(x) else x)\n",
    "\n",
    "train_test.drop(['description', 'desc'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# address\n",
    "train_test['address1'] = train_test['display_address']\n",
    "train_test['address1'] = train_test['address1'].apply(lambda x: x.lower())\n",
    "\n",
    "address_map = {\n",
    "    'w': 'west',\n",
    "    'st.': 'street',\n",
    "    'ave': 'avenue',\n",
    "    'st': 'street',\n",
    "    'e': 'east',\n",
    "    'n': 'north',\n",
    "    's': 'south'\n",
    "}\n",
    "\n",
    "def address_map_func(s):\n",
    "    s = s.split(' ')\n",
    "    out = []\n",
    "    for x in s:\n",
    "        if x in address_map:\n",
    "            out.append(address_map[x])\n",
    "        else:\n",
    "            out.append(x)\n",
    "    return ' '.join(out)\n",
    "\n",
    "\n",
    "train_test['address1'] = train_test['address1'].apply(lambda x: x.translate(remove_punct_map))\n",
    "train_test['address1'] = train_test['address1'].apply(lambda x: address_map_func(x))\n",
    "\n",
    "new_cols = ['street', 'avenue', 'east', 'west', 'north', 'south']\n",
    "for col in new_cols:\n",
    "    train_test[col] = train_test['address1'].apply(lambda x: 1 if col in x else 0)\n",
    "\n",
    "train_test['other_address'] = train_test[new_cols].apply(lambda x: 1 if x.sum() == 0 else 0, axis=1)\n",
    "train_test.drop(['display_address', 'street_address'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# features (better not to lower)\n",
    "train_test['features_count'] = train_test['features'].apply(lambda x: len(x))\n",
    "train_test['features2'] = train_test['features']\n",
    "train_test['features2'] = train_test['features2'].apply(lambda x: ' '.join([''.join(i.replace('_',' ').replace('-',' ').split(' ')) for i in x]))\n",
    "\n",
    "c_vect = CountVectorizer(stop_words='english', max_features=200, ngram_range=(1, 1))\n",
    "c_vect.fit(train_test['features2'])\n",
    "\n",
    "c_vect_sparse_1 = c_vect.transform(train_test['features2'])\n",
    "c_vect_sparse1_cols = c_vect.get_feature_names()\n",
    "train_test.drop(['features', 'features2'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# photos\n",
    "train_test['photos_count'] = train_test['photos'].apply(lambda x: len(x))\n",
    "train_test.drop(['photos'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# convert ['building_id', 'manager_id', 'address1'] to enumerated labels\n",
    "categoricals = [x for x in train_test.columns if train_test[x].dtype == 'object']\n",
    "for feat in categoricals:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train_test[feat].values))\n",
    "    train_test[feat] = lbl.transform(list(train_test[feat].values))\n",
    "\n",
    "\n",
    "# convert bed/bath number to enumeration\n",
    "train_test['bathrooms_cat'] = train_test['bathrooms'].apply(lambda x: str(x))\n",
    "train_test['bathrooms_cat'], labels = pd.factorize(train_test['bathrooms_cat'].values, sort=True)\n",
    "train_test.drop('bathrooms', axis=1, inplace=True)\n",
    "train_test['bedroom_cat'], labels = pd.factorize(train_test['bedrooms'].values, sort=True)\n",
    "train_test.drop('bedrooms', axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "# try adding real - predicted price\n",
    "# Try to predict price for a listing and add real_price - expected_price as a feature\n",
    "# train on [exclusive, byowner, nofee] and #bed, #bath, display address\n",
    "features_to_use = ['bathrooms_cat', 'bedroom_cat', 'street', 'avenue', 'east', 'west', 'north', 'south', 'address1', 'other_address', 'building_worths', 'manager_worths']\n",
    "feature_to_predict = 'price'\n",
    "params = {\n",
    "    'objective': 'reg:linear',\n",
    "    'booster':'gblinear',\n",
    "    'lambda': 0,\n",
    "    'lambda_bias' : 0,\n",
    "    'alpha': 0.2\n",
    "}\n",
    "prices = train_test[feature_to_predict]\n",
    "remove_outliers = np.abs(prices-prices.mean())<=(3*prices.std())\n",
    "dtrain = xgb.DMatrix(data=train_test[remove_outliers][features_to_use], label=train_test[remove_outliers][feature_to_predict])\n",
    "\n",
    "bst = xgb.cv(params, dtrain, 10000, 4, early_stopping_rounds=50, verbose_eval=200)\n",
    "best_rounds = np.argmin(bst['test-rmse-mean'])\n",
    "print (bst['test-rmse-mean'][best_rounds])\n",
    "bst = xgb.train(params, dtrain, best_rounds)\n",
    "dtrain = xgb.DMatrix(data=train_test[features_to_use])\n",
    "expected_price = bst.predict(dtrain)\n",
    "train_test['real_minus_expected_price'] = train_test[feature_to_predict] - expected_price\n",
    "train_test['real_over_expected_price'] = train_test[feature_to_predict] / expected_price\n",
    "\n",
    "\n",
    "# transform price\n",
    "bc_price, tmp = boxcox(train_test.price)\n",
    "train_test['bc_price'] = bc_price\n",
    "train_test.drop('price', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# add sparse\n",
    "train_test_cv1_sparse = sparse.hstack((train_test, c_vect_sparse_1)).tocsr()\n",
    "#x_train = train_test_cv1_sparse[:20000, :]\n",
    "#y_train = y_train[:20000]\n",
    "x_train = train_test_cv1_sparse[:ntrain, :]\n",
    "x_test = train_test_cv1_sparse[ntrain:, :]\n",
    "\n",
    "\n",
    "# add feature names\n",
    "features = list(train_test.columns) + ['sparse_' + vect_name for vect_name in c_vect_sparse1_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09152+1.28062e-05\ttest-mlogloss:1.09169+3.0288e-05\n",
      "[200]\ttrain-mlogloss:0.592627+0.000429155\ttest-mlogloss:0.621589+0.00117747\n",
      "[400]\ttrain-mlogloss:0.511057+0.000422405\ttest-mlogloss:0.562877+0.00159155\n",
      "[600]\ttrain-mlogloss:0.473634+0.000149098\ttest-mlogloss:0.545578+0.00183562\n",
      "[800]\ttrain-mlogloss:0.447848+0.000361501\ttest-mlogloss:0.537699+0.00185401\n",
      "[1000]\ttrain-mlogloss:0.426777+0.000583778\ttest-mlogloss:0.533297+0.00170786\n",
      "[1200]\ttrain-mlogloss:0.408599+0.000783595\ttest-mlogloss:0.530551+0.00151466\n",
      "[1400]\ttrain-mlogloss:0.392322+0.000898563\ttest-mlogloss:0.528749+0.0015258\n",
      "[1600]\ttrain-mlogloss:0.377225+0.000903515\ttest-mlogloss:0.527561+0.00145174\n",
      "[1800]\ttrain-mlogloss:0.363205+0.00088881\ttest-mlogloss:0.526868+0.00141634\n",
      "[2000]\ttrain-mlogloss:0.349847+0.000872462\ttest-mlogloss:0.526541+0.00144961\n",
      "0.5265242\n",
      "CPU times: user 51min 56s, sys: 5min 46s, total: 57min 42s\n",
      "Wall time: 21min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SEED = 777\n",
    "NFOLDS = 5\n",
    "\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.8,\n",
    "    'subsample':.8,\n",
    "    'seed':0,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=x_test)\n",
    "\n",
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200)\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "print (bst['test-mlogloss-mean'][best_rounds])\n",
    "#bst_cv_scores.append(bst['test-mlogloss-mean'][best_rounds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.553719894183\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(params, dtrain, best_rounds)\n",
    "preds = bst.predict(dtest)\n",
    "print (log_loss(x_test, preds))\n",
    "\n",
    "#preds = pd.DataFrame(preds)\n",
    "#cols = ['high', 'medium', 'low']\n",
    "#preds.columns = cols\n",
    "#preds['listing_id'] = listing_id\n",
    "#preds.to_csv('my_preds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance = bst.get_fscore()\n",
    "feature_importance = [(feature, (importance['f{}'.format(i)]) if 'f{}'.format(i) in importance else 0) for i, feature in enumerate(features)]\n",
    "sorted(feature_importance, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
