{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.metrics import log_loss\n",
    "import string\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from scipy.stats import boxcox\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class NBatchLogger(Callback):\n",
    "    def __init__(self):\n",
    "        self.seen = 0\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.seen += 1\n",
    "        if self.seen % 50 == 0:\n",
    "            print('epoch {}: loss = {}, val = {}'.format(self.seen, logs['loss'], logs['val_loss'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = pd.read_json('data/train_with_leaky_cv.json')\n",
    "train_y = pd.read_json('data/train_interest.json')[0]\n",
    "columns = train_X.columns\n",
    "train_X_scaled = pd.DataFrame(StandardScaler().fit_transform(train_X), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_usefulness = [('bc_price', 16597), ('manager_worths', 14684), ('price_by_NTACode', 13343), ('latitude', 12299), ('longitude', 12051), ('desc_words_length', 11477), ('manager_id', 11136), ('price_by_address1', 11125), ('Yday', 10884), ('address1', 10458), ('building_worths', 10298), ('building_id', 10142), ('desc_letters_count', 9832), ('real_minus_expected_price', 9815), ('price_by_area', 9742), ('real_over_expected_price', 9374), ('Day', 9066), ('worth_by_address1', 8782), ('hour', 7722), ('price_per_bathroom', 7590), ('desc_words_count', 7481), ('price_per_bed_and_bath', 7387), ('worth_by_NTACode', 6993), ('photos_count', 6730), ('NTACode', 6324), ('price_by_BoroCode', 6098), ('features_count', 5668), ('Wday', 4445), ('price_per_bedroom', 3903), ('sparse_nofee', 3056), ('bedrooms', 2833), ('sparse_furnished', 1865), ('bathrooms', 1702), ('sparse_laundryinbuilding', 1425), ('sparse_hardwoodfloors', 1132), ('sparse_laundryinunit', 1088), ('Month', 1006), ('sparse_exclusive', 771), ('sparse_dogsallowed', 770), ('BoroCode', 749), ('sparse_catsallowed', 718), ('sparse_prewar', 663), ('sparse_privateoutdoorspace', 634), ('sparse_reducedfee', 631), ('avenue', 595), ('street', 569), ('other_address', 553), ('sparse_commonoutdoorspace', 545), ('Zero_building_id', 534), ('sparse_elevator', 533), ('east', 524), ('sparse_doorman', 510), ('sparse_parkingspace', 506), ('sparse_dishwasher', 500), ('sparse_highspeedinternet', 476), ('sparse_terrace', 424), ('sparse_fitnesscenter', 423), ('sparse_loft', 413), ('sparse_outdoorspace', 406), ('sparse_actualapt', 399), ('sparse_roofdeck', 359), ('sparse_balcony', 346), ('sparse_diningroom', 318), ('sparse_simplex', 292), ('sparse_swimmingpool', 283), ('sparse_shorttermallowed', 263), ('sparse_wheelchairaccess', 248), ('west', 245), ('sparse_newconstruction', 238), ('sparse_garden', 231), ('sparse_multilevel', 201), ('sparse_patio', 199), ('sparse_hardwood', 198), ('worth_by_BoroCode', 179), ('sparse_fireplace', 168), ('sparse_highceiling', 159), ('sparse_petsok', 154), ('sparse_dryerinunit', 153), ('sparse_stainlesssteelappliances', 148), ('sparse_garage', 119), ('south', 118), ('sparse_storage', 118), ('sparse_outdoorareas', 117), ('sparse_lndrybldg', 113), ('sparse_liveinsuper', 110), ('sparse_new', 103), ('sparse_renovated', 99), ('sparse_photos', 88), ('sparse_highceilings', 79), ('sparse_onsitelaundry', 79), ('sparse_view', 78), ('sparse_flex3', 67), ('sparse_centrala', 66), ('sparse_concierge', 64), ('sparse_residentslounge', 63), ('sparse_assignedparkingspace', 62), ('sparse_allutilitiesincluded', 58), ('sparse_onsitegarage', 53), ('north', 46), ('sparse_lowrise', 44), ('sparse_washerinunit', 44)]\n",
    "leaky = ['manager_worths', 'building_worths', 'price_by_area', 'real_minus_expected_price', 'real_over_expected_price', 'price_by_address1', 'worth_by_address1', 'price_by_BoroCode', 'worth_by_BoroCode', 'price_by_NTACode', 'worth_by_NTACode']\n",
    "all_columns = ['BoroCode', 'Day', 'Month', 'NTACode', 'Wday', 'Yday', 'Zero_building_id', 'Zero_manager_id', 'address1', 'avenue', 'bathrooms', 'bc_price', 'bedrooms', 'building_id', 'building_worths', 'desc_letters_count', 'desc_words_count', 'desc_words_length', 'east', 'features_count', 'hour', 'latitude', 'longitude', 'manager_id', 'manager_worths', 'north', 'other_address', 'photos_count', 'price_by_BoroCode', 'price_by_NTACode', 'price_by_address1', 'price_by_area', 'price_per_bathroom', 'price_per_bed_and_bath', 'price_per_bedroom', 'real_minus_expected_price', 'real_over_expected_price', 'south', 'sparse_24', 'sparse_2fullbaths', 'sparse_actualapt', 'sparse_airconditioning', 'sparse_allutilitiesincluded', 'sparse_assignedparkingspace', 'sparse_attendedlobby', 'sparse_backyard', 'sparse_balconies', 'sparse_balcony', 'sparse_basementstorage', 'sparse_bikeroom', 'sparse_bikestorage', 'sparse_billiardsroom', 'sparse_brownstone', 'sparse_buildingcommonoutdoorspace', 'sparse_businesscenter', 'sparse_cable', 'sparse_catsallowed', 'sparse_centrala', 'sparse_centralac', 'sparse_centralair', 'sparse_chef', 'sparse_chefskitchen', 'sparse_children', 'sparse_childrensplayroom', 'sparse_cityview', 'sparse_commonbackyard', 'sparse_commongarden', 'sparse_commonoutdoorspace', 'sparse_commonparking', 'sparse_commonroofdeck', 'sparse_commonterrace', 'sparse_communityrecreationfacilities', 'sparse_concierge', 'sparse_conciergeservice', 'sparse_condofinishes', 'sparse_cook', 'sparse_courtyard', 'sparse_deck', 'sparse_decorativefireplace', 'sparse_diningroom', 'sparse_dishwasher', 'sparse_dogsallowed', 'sparse_doorman', 'sparse_drycleaningservice', 'sparse_dryer', 'sparse_dryerinbuilding', 'sparse_dryerinunit', 'sparse_duplex', 'sparse_eatinkitchen', 'sparse_elev', 'sparse_elevator', 'sparse_elevbldg', 'sparse_exclusive', 'sparse_exposedbrick', 'sparse_fireplace', 'sparse_fireplaces', 'sparse_fitness', 'sparse_fitnesscenter', 'sparse_flex2', 'sparse_flex3', 'sparse_ftdoorman', 'sparse_fullservicegarage', 'sparse_fulltimedoorman', 'sparse_furnished', 'sparse_garage', 'sparse_garden', 'sparse_gourmetkitchen', 'sparse_granitecountertops', 'sparse_granitekitchen', 'sparse_greenbuilding', 'sparse_guarantorsaccepted', 'sparse_gutrenovated', 'sparse_gym', 'sparse_gyminbuilding', 'sparse_hardwood', 'sparse_hardwoodfloors', 'sparse_healthclub', 'sparse_highceiling', 'sparse_highceilings', 'sparse_highrise', 'sparse_highspeedinternet', 'sparse_hirise', 'sparse_housekeeping', 'sparse_indoorpool', 'sparse_inunitwasher', 'sparse_largelivingroom', 'sparse_laundry', 'sparse_laundryinbuilding', 'sparse_laundryinunit', 'sparse_laundryonfloor', 'sparse_laundryroom', 'sparse_light', 'sparse_live', 'sparse_liveinsuper', 'sparse_liveinsuperintendent', 'sparse_lndrybldg', 'sparse_loft', 'sparse_lounge', 'sparse_loungeroom', 'sparse_lowrise', 'sparse_luxurybuilding', 'sparse_mailroom', 'sparse_marblebath', 'sparse_marblebathroom', 'sparse_microwave', 'sparse_midrise', 'sparse_mrcleanapproved', 'sparse_multilevel', 'sparse_new', 'sparse_newconstruction', 'sparse_newlyrenovated', 'sparse_nofee', 'sparse_nopets', 'sparse_nursery', 'sparse_onsitegarage', 'sparse_onsitelaundry', 'sparse_onsiteparking', 'sparse_onsiteparkingavailable', 'sparse_onsiteparkinglot', 'sparse_onsitesuper', 'sparse_ornateprewardetails', 'sparse_outdoorareas', 'sparse_outdoorentertainmentspace', 'sparse_outdoorpool', 'sparse_outdoorspace', 'sparse_packageroom', 'sparse_parking', 'sparse_parkingspace', 'sparse_patio', 'sparse_penthouse', 'sparse_petfriendly', 'sparse_pets', 'sparse_petsallowed', 'sparse_petsok', 'sparse_petsonapproval', 'sparse_photos', 'sparse_playroom', 'sparse_pool', 'sparse_postwar', 'sparse_prewar', 'sparse_privatebackyard', 'sparse_privatebalcony', 'sparse_privatedeck', 'sparse_privatelaundryroomoneveryfloor', 'sparse_privateoutdoorspace', 'sparse_privateparking', 'sparse_privateroofdeck', 'sparse_privateterrace', 'sparse_publicoutdoor', 'sparse_reducedfee', 'sparse_renovated', 'sparse_residentsgarden', 'sparse_residentslounge', 'sparse_roofaccess', 'sparse_roofdeck', 'sparse_rooftopdeck', 'sparse_rooftopterrace', 'sparse_roomyclosets', 'sparse_satellitetv', 'sparse_sauna', 'sparse_screeningroom', 'sparse_sharedbackyard', 'sparse_sharesok', 'sparse_shorttermallowed', 'sparse_simplex', 'sparse_skitchen', 'sparse_skylight', 'sparse_splayroom', 'sparse_sskitchen', 'sparse_stainlesssteelappliances', 'sparse_stepstothepark', 'sparse_storage', 'sparse_storagefacilitiesavailable', 'sparse_storageroom', 'sparse_sublet', 'sparse_subway', 'sparse_sundeck', 'sparse_swimmingpool', 'sparse_tenantlounge', 'sparse_terrace', 'sparse_terraces', 'sparse_tonsofnaturallight', 'sparse_valet', 'sparse_valetparking', 'sparse_valetservices', 'sparse_valetservicesincludingdrycleaning', 'sparse_videointercom', 'sparse_view', 'sparse_virtualdoorman', 'sparse_walkincloset', 'sparse_wallsofwindows', 'sparse_washer', 'sparse_washerinunit', 'sparse_wheelchairaccess', 'sparse_wheelchairramp', 'sparse_wifi', 'sparse_wifiaccess', 'sparse_work', 'street', 'west', 'worth_by_BoroCode', 'worth_by_NTACode', 'worth_by_address1']\n",
    "sparse_cols = [x[0] for x in column_usefulness if x[0].startswith('sparse')][:15]\n",
    "non_sparse_cols = [x[0] for x in column_usefulness if not x[0].startswith('sparse')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a bunch of features to train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = pd.read_json('data/train_with_leaky_cv.json')\n",
    "train_y = pd.read_json('data/train_interest.json')[0]\n",
    "columns = train_X.columns\n",
    "train_X_scaled = pd.DataFrame(StandardScaler().fit_transform(train_X), columns=columns)\n",
    "train_X_with_layer_1 = pd.read_json('data/train_scaled_with_leaky_and_added_features_cv.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_features = ['bc_price', 'manager_worths', 'latitude', 'longitude', 'desc_words_length', 'Yday', 'building_worths', 'price_per_bedroom']\n",
    "knn_n_closest = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "n_components = 3\n",
    "ridge_columns = ['layer1_ridge_predictions']\n",
    "pca_columns = ['layer1_pca_{}'.format(i) for i in range(n_components)]\n",
    "knn_columns = ['layer1_knn_{}_{}'.format(n_neighbors, category) for category in ['high', 'medium', 'low'] for n_neighbors in knn_n_closest]\n",
    "rf_columns = ['layer1_rf_{}_{}_predictions'.format(criterion, category) for category in ['high', 'medium', 'low'] for criterion in ['gini', 'entropy']]\n",
    "k_means_columns = ['layer1_k_means_{}'.format(n_clusters) for n_clusters in [2, 4, 8]]\n",
    "new_columns_generated = ridge_columns + pca_columns + knn_columns + rf_columns + k_means_columns\n",
    "for column in knn_columns:\n",
    "    train_X_with_layer_1[column] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#PCA\n",
    "\n",
    "pca_cols = ['layer1_pca_{}'.format(i) for i in range(n_components)]\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_representation = pca.fit_transform(train_X_scaled)\n",
    "train_X_with_layer_1.loc[:, pca_cols] = pca_representation\n",
    "\n",
    "\n",
    "#tsne = TSNE(n_components = 2)\n",
    "#tsne_representation = tsne.fit_transform(train_X_scaled[knn_features[:15]][:5000])\n",
    "\n",
    "folds = 5\n",
    "full_length = len(train_X_scaled)\n",
    "entries = int(full_length / folds)\n",
    "full_index = train_X_scaled.index\n",
    "fold_indices = []\n",
    "fold_indices_keep = []\n",
    "fold_indices_throw = []\n",
    "for column in leaky:\n",
    "    train_X_scaled[column] = 0\n",
    "for i in range(folds):\n",
    "    print ('Fold {}'.format(i))\n",
    "    indices = sorted(random.sample(list(full_index), entries))\n",
    "    fold_indices.append(indices)\n",
    "    full_index = full_index.drop(indices)\n",
    "\n",
    "    full_index_keep = np.zeros((full_length), dtype=np.bool)\n",
    "    full_index_throw = np.ones((full_length), dtype=np.bool)\n",
    "    for j in fold_indices[i]:\n",
    "        full_index_keep[j] = True\n",
    "        full_index_throw[j] = False\n",
    "    fold_indices_throw.append(full_index_throw)\n",
    "    fold_indices_keep.append(full_index_keep)\n",
    "    \n",
    "    xtr = train_X_scaled[full_index_throw]\n",
    "    ytr = train_y[full_index_throw]\n",
    "    xte = train_X_scaled[full_index_keep]\n",
    "    \n",
    "    for n_neighbors in knn_n_closest:\n",
    "        #knn_dist = DistanceMetric.get_metric('mahalanobis', V=np.eye(len(knn_n_closest)))\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric='manhattan') # metric='manhattan'\n",
    "        knn.fit(train_X_scaled[knn_features][full_index_throw], train_y[full_index_throw])\n",
    "        knn_pred = knn.predict_proba(train_X_scaled[knn_features][full_index_keep])\n",
    "        new_cols = ['layer1_knn_{}_{}'.format(n_neighbors, category) for category in ['high', 'medium', 'low']]\n",
    "        train_X_with_layer_1.loc[indices, new_cols] = knn_pred\n",
    "    \n",
    "    '''\n",
    "    ridge = Ridge()\n",
    "    ridge.fit(xtr, ytr)\n",
    "    ridge_predictions = ridge.predict(xte)\n",
    "    train_X_with_layer_1.loc[indices, 'layer1_ridge_predictions'] = ridge_predictions\n",
    "    \n",
    "    for criterion in ['gini', 'entropy']:\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=25,\n",
    "            criterion=criterion,\n",
    "            max_features='auto',\n",
    "            min_samples_split=20)\n",
    "\n",
    "        rf.fit(xtr, ytr)\n",
    "        rf_predictions = rf.predict_proba(xte)\n",
    "        new_cols = ['layer1_rf_{}_{}_predictions'.format(criterion, category) for category in ['high', 'medium', 'low']]\n",
    "        train_X_with_layer_1.loc[indices, new_cols] = rf_predictions\n",
    "    \n",
    "    for n_clusters in [2, 4, 8]:\n",
    "        k_means = KMeans(n_clusters=n_clusters)\n",
    "        k_means.fit(xtr[knn_features], ytr)\n",
    "        k_means_indices = k_means.predict(xte[knn_features])\n",
    "        train_X_with_layer_1.loc[indices, 'layer1_k_means_{}'.format(n_clusters)] = k_means_indices\n",
    "    \n",
    "    for n_neighbors in [2, 4, 8, 16, 64, 256, 1024]:\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric='manhattan') # mahalanobis\n",
    "        knn.fit(train_X_scaled[knn_features][full_index_throw], train_y[full_index_throw])\n",
    "        knn_pred = knn.predict(train_X_scaled[knn_features][full_index_keep])\n",
    "        train_X_with_layer_1.loc[indices, 'layer1_knn_{}'.format(n_neighbors)] = knn_pred\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_with_layer_1.to_json('data/train_scaled_with_leaky_and_added_features_cv.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50: loss = 0.5914606363581266, val = 0.5958407469459747\n",
      "epoch 100: loss = 0.568360930373012, val = 0.5813010576421002\n",
      "epoch 150: loss = 0.5530395628425995, val = 0.5782067945390374\n",
      "epoch 200: loss = 0.5436676874319609, val = 0.5766880485184055\n",
      "epoch 250: loss = 0.534290305472608, val = 0.5763892252067632\n",
      "0.57560832226\n",
      "epoch 50: loss = 0.5907035735841637, val = 0.5942039608588229\n",
      "epoch 100: loss = 0.564317243756355, val = 0.5821416626913386\n",
      "epoch 150: loss = 0.5488034374498629, val = 0.5774871603772829\n",
      "epoch 200: loss = 0.539257666988917, val = 0.57818092279949\n",
      "0.576082880874\n",
      "epoch 50: loss = 0.5855864123006742, val = 0.5909427438317174\n",
      "epoch 100: loss = 0.5643452853375035, val = 0.5818857556997927\n",
      "epoch 150: loss = 0.5508964615383598, val = 0.579674539863242\n",
      "0.578340048417\n",
      "epoch 50: loss = 0.5909714835250617, val = 0.5927352374928333\n",
      "epoch 100: loss = 0.5665478209976555, val = 0.5811703168876284\n",
      "epoch 150: loss = 0.5495527357679457, val = 0.5770774268025729\n",
      "epoch 200: loss = 0.5402205075090165, val = 0.5760174333363226\n",
      "0.576017433395\n",
      "epoch 50: loss = 0.5915007807997578, val = 0.5930829558791441\n",
      "epoch 100: loss = 0.565323807798741, val = 0.58230728543854\n",
      "epoch 150: loss = 0.5518447738538371, val = 0.5781600178978898\n",
      "epoch 200: loss = 0.5407207971673773, val = 0.5764713155177873\n",
      "0.575663257464\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    test_nn_with_columns(train_X_with_layer_1, train_y, all_columns + [x for x in knn_columns if int(x.split('_')[2]) > 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[x for x in knn_columns if int(x.split('_')[2]) > 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50: loss = 0.5938834688481451, val = 0.5979886952202486\n",
      "epoch 100: loss = 0.5703038286433625, val = 0.5848430726145167\n",
      "epoch 150: loss = 0.5516893069625719, val = 0.5780559380333898\n",
      "epoch 200: loss = 0.5443068869645845, val = 0.5776463041991154\n",
      "epoch 250: loss = 0.5371102081381174, val = 0.5770619802042892\n",
      "epoch 300: loss = 0.529178605043193, val = 0.5779906169273301\n",
      "0.575908581058\n",
      "epoch 50: loss = 0.5957561787088023, val = 0.6010609661514436\n",
      "epoch 100: loss = 0.5700007426318121, val = 0.5846389501557433\n",
      "epoch 150: loss = 0.551885164965363, val = 0.579517080092434\n",
      "epoch 200: loss = 0.5435316820433739, val = 0.5773379306755339\n",
      "0.576451235521\n",
      "epoch 50: loss = 0.5939590478340205, val = 0.5995870211519014\n",
      "epoch 100: loss = 0.5693623903324985, val = 0.5835657462170917\n",
      "epoch 150: loss = 0.5535141397586626, val = 0.5789996878720891\n",
      "epoch 200: loss = 0.5406997339255408, val = 0.5785544408664341\n",
      "epoch 250: loss = 0.5346248421083105, val = 0.5785859360902038\n",
      "0.576419783162\n",
      "epoch 50: loss = 0.5970918871681752, val = 0.6016637808133763\n",
      "epoch 100: loss = 0.5704829649603296, val = 0.5851202381644649\n",
      "epoch 150: loss = 0.5526353001848975, val = 0.5793659266650262\n",
      "epoch 200: loss = 0.5433719437499934, val = 0.5787980413490996\n",
      "0.576687569416\n",
      "epoch 50: loss = 0.5940204417131615, val = 0.601126092327416\n",
      "epoch 100: loss = 0.5685731733599584, val = 0.58348968562749\n",
      "epoch 150: loss = 0.5577511861747246, val = 0.5806487173658461\n",
      "epoch 200: loss = 0.5417587848142383, val = 0.5767502637673939\n",
      "0.57612033989\n"
     ]
    }
   ],
   "source": [
    "# after adding others\n",
    "for i in range(5):\n",
    "    test_nn_with_columns(train_X_with_layer_1, train_y, all_columns + pca_cols)\n",
    "# 0.5761ish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09099+2.86744e-05\ttest-mlogloss:1.09118+3.23144e-05\n",
      "[200]\ttrain-mlogloss:0.574909+0.00234793\ttest-mlogloss:0.608062+0.00333607\n",
      "[400]\ttrain-mlogloss:0.501247+0.00298666\ttest-mlogloss:0.559249+0.00449265\n",
      "[600]\ttrain-mlogloss:0.469505+0.00322885\ttest-mlogloss:0.549452+0.00487377\n",
      "[800]\ttrain-mlogloss:0.445462+0.00329606\ttest-mlogloss:0.545648+0.00518052\n",
      "[1000]\ttrain-mlogloss:0.424577+0.00313616\ttest-mlogloss:0.543703+0.00533344\n",
      "[1200]\ttrain-mlogloss:0.405652+0.00262246\ttest-mlogloss:0.542455+0.00544267\n",
      "[1400]\ttrain-mlogloss:0.388198+0.00251352\ttest-mlogloss:0.541651+0.00548474\n",
      "[1600]\ttrain-mlogloss:0.372008+0.00211506\ttest-mlogloss:0.541326+0.00552556\n",
      "[1800]\ttrain-mlogloss:0.356742+0.00195141\ttest-mlogloss:0.541163+0.00552586\n",
      "0.541107666667\n",
      "1867\n",
      "CPU times: user 2h 9min 27s, sys: 3min 24s, total: 2h 12min 52s\n",
      "Wall time: 40min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SEED = 777\n",
    "NFOLDS = 3\n",
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.8,\n",
    "    'subsample':.8,\n",
    "    'seed':0,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "extra_cols_to_use = [x for x in knn_columns if int(x.split('_')[2]) > 128] + rf_columns + pca_columns\n",
    "dtrain = xgb.DMatrix(data=train_X_with_layer_1[all_columns + extra_cols_to_use], label=train_y)\n",
    "\n",
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200)\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "print (bst['test-mlogloss-mean'][best_rounds])\n",
    "print (best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09099+3.72499e-05\ttest-mlogloss:1.0912+1.666e-05\n",
      "[200]\ttrain-mlogloss:0.574327+0.00236692\ttest-mlogloss:0.60816+0.0033186\n",
      "[400]\ttrain-mlogloss:0.499712+0.00309028\ttest-mlogloss:0.559459+0.00472332\n",
      "[600]\ttrain-mlogloss:0.46717+0.00367726\ttest-mlogloss:0.549559+0.00507385\n",
      "[800]\ttrain-mlogloss:0.442664+0.00379257\ttest-mlogloss:0.545771+0.00517363\n",
      "[1000]\ttrain-mlogloss:0.421248+0.00340619\ttest-mlogloss:0.543869+0.00533699\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SEED = 777\n",
    "NFOLDS = 3\n",
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.8,\n",
    "    'subsample':.8,\n",
    "    'seed':0,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "extra_cols_to_use = knn_columns + rf_columns + pca_columns\n",
    "dtrain = xgb.DMatrix(data=train_X_with_layer_1[all_columns + extra_cols_to_use], label=train_y)\n",
    "\n",
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200)\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "print (bst['test-mlogloss-mean'][best_rounds])\n",
    "print (best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SEED = 777\n",
    "NFOLDS = 3\n",
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.8,\n",
    "    'subsample':.8,\n",
    "    'seed':0,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "extra_cols_to_use = knn_columns + rf_columns + pca_columns + k_means_columns\n",
    "dtrain = xgb.DMatrix(data=train_X_with_layer_1[all_columns + extra_cols_to_use], label=train_y)\n",
    "\n",
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200)\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "print (bst['test-mlogloss-mean'][best_rounds])\n",
    "print (best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SEED = 777\n",
    "NFOLDS = 3\n",
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.8,\n",
    "    'subsample':.8,\n",
    "    'seed':0,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "extra_cols_to_use = knn_columns + rf_columns + pca_columns + ridge_columns\n",
    "dtrain = xgb.DMatrix(data=train_X_with_layer_1[all_columns + extra_cols_to_use], label=train_y)\n",
    "\n",
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200)\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "print (bst['test-mlogloss-mean'][best_rounds])\n",
    "print (best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SEED = 777\n",
    "NFOLDS = 3\n",
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.8,\n",
    "    'subsample':.8,\n",
    "    'seed':0,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "extra_cols_to_use = [x for x in train_X_with_layer_1.columns if x.startswith('layer1_knn')] + rf_columns + pca_columns + ridge_columns + k_means_columns\n",
    "dtrain = xgb.DMatrix(data=train_X_with_layer_1[all_columns + extra_cols_to_use], label=train_y)\n",
    "\n",
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200)\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "print (bst['test-mlogloss-mean'][best_rounds])\n",
    "print (best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50: loss = 0.5891488493100785, val = 0.5963193890939831\n",
      "epoch 100: loss = 0.5633413129618845, val = 0.5837058686305724\n",
      "epoch 150: loss = 0.5528043889919517, val = 0.5779784439371053\n",
      "epoch 200: loss = 0.5419338218633879, val = 0.5762588906121962\n",
      "0.575842619402\n",
      "epoch 50: loss = 0.5988605105555501, val = 0.6008190141541198\n",
      "epoch 100: loss = 0.5726395071071351, val = 0.5853478953466943\n",
      "epoch 150: loss = 0.5545519629186689, val = 0.5786513815457169\n",
      "epoch 200: loss = 0.5445255741711316, val = 0.577489306096185\n",
      "0.576257448063\n",
      "epoch 50: loss = 0.5970482967136526, val = 0.6019688100613078\n",
      "epoch 100: loss = 0.5716081300411502, val = 0.585956075933923\n",
      "epoch 150: loss = 0.5586140227548408, val = 0.5794861443735712\n",
      "epoch 200: loss = 0.544868941424815, val = 0.5781161536233587\n",
      "epoch 250: loss = 0.5349255880409272, val = 0.5778112565414081\n",
      "0.576570762645\n",
      "epoch 50: loss = 0.5934883482279587, val = 0.5983995749452238\n",
      "epoch 100: loss = 0.5701759461581807, val = 0.5851851115738443\n",
      "epoch 150: loss = 0.5549327430903136, val = 0.5796144609127425\n",
      "epoch 200: loss = 0.5427235050223832, val = 0.5778040029682203\n",
      "epoch 250: loss = 0.5338250904391275, val = 0.5776341136241195\n",
      "epoch 300: loss = 0.5287631308748378, val = 0.5778935201300622\n",
      "0.575981362863\n",
      "epoch 50: loss = 0.5924579278540429, val = 0.5966187789734265\n",
      "epoch 100: loss = 0.5674167989995289, val = 0.581410089194591\n",
      "epoch 150: loss = 0.5499542500080369, val = 0.5777095916843584\n",
      "epoch 200: loss = 0.5407068069907301, val = 0.5777259109220816\n",
      "0.575938266543\n"
     ]
    }
   ],
   "source": [
    "# after adding others\n",
    "for i in range(5):\n",
    "    test_nn_with_columns(train_X, train_y, all_columns)\n",
    "# 0.5761ish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50: loss = 0.5660618697845898, val = 0.5738794634921525\n",
      "epoch 100: loss = 0.5476543278769387, val = 0.5692151620836733\n",
      "0.568399167234\n",
      "epoch 50: loss = 0.5654138053050335, val = 0.5748133755692556\n",
      "epoch 100: loss = 0.5478983607211402, val = 0.570332626475954\n",
      "0.570071592402\n",
      "epoch 50: loss = 0.5674681716364117, val = 0.5733706152097084\n",
      "epoch 100: loss = 0.5466272889740071, val = 0.5702550949975377\n",
      "0.569464498692\n",
      "epoch 50: loss = 0.5714001220983553, val = 0.5742717927134893\n",
      "epoch 100: loss = 0.5496349390401906, val = 0.5703977996670344\n",
      "0.569792896392\n",
      "epoch 50: loss = 0.5662333334826274, val = 0.5737657303515623\n",
      "epoch 100: loss = 0.5473061113198573, val = 0.5703907628309826\n",
      "epoch 150: loss = 0.5327434986531847, val = 0.5713913049491497\n",
      "0.569260921208\n"
     ]
    }
   ],
   "source": [
    "# after adding others using all columns\n",
    "for i in range(5):\n",
    "    test_nn_with_columns(train_X, train_y, all_columns + new_columns_generated)\n",
    "# 0.5698982835840147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50: loss = 0.5968368610190747, val = 0.5983693936903408\n",
      "epoch 100: loss = 0.5712352800570232, val = 0.5848432007611213\n",
      "epoch 150: loss = 0.5580001741948851, val = 0.5787365255295545\n",
      "epoch 200: loss = 0.5492674991075408, val = 0.5781092188369514\n",
      "epoch 250: loss = 0.5349007071935745, val = 0.5770544120665984\n",
      "0.576137658997\n",
      "epoch 50: loss = 0.5948694923302533, val = 0.6006885247169467\n",
      "epoch 100: loss = 0.5696178057652763, val = 0.5842879144506568\n",
      "epoch 150: loss = 0.554824017569203, val = 0.5798235199481978\n",
      "epoch 200: loss = 0.546429100270827, val = 0.577611118289365\n",
      "0.576528663307\n",
      "epoch 50: loss = 0.5957544322833704, val = 0.6000883799450268\n",
      "epoch 100: loss = 0.574421041257675, val = 0.5853944098974554\n",
      "epoch 150: loss = 0.554906663914879, val = 0.5792153256796306\n",
      "epoch 200: loss = 0.5423362196271189, val = 0.5782430684630266\n",
      "epoch 250: loss = 0.5353194022736467, val = 0.5760248840811728\n",
      "0.575659649132\n",
      "epoch 50: loss = 0.59418502251436, val = 0.5983718875007549\n",
      "epoch 100: loss = 0.5706177460140532, val = 0.5848504713103767\n",
      "epoch 150: loss = 0.5541667848054108, val = 0.5788555367682041\n",
      "epoch 200: loss = 0.5402530072129204, val = 0.5773197835974647\n",
      "epoch 250: loss = 0.5347926110993807, val = 0.5760800171601829\n",
      "0.575267976357\n",
      "epoch 50: loss = 0.5931401433360862, val = 0.5974267507067065\n",
      "epoch 100: loss = 0.5717591473669379, val = 0.5847402345440457\n",
      "epoch 150: loss = 0.5531635692448491, val = 0.5788458202255856\n",
      "epoch 200: loss = 0.5412582811933351, val = 0.5788333846715382\n",
      "epoch 250: loss = 0.53562443103848, val = 0.5774225997642676\n",
      "epoch 300: loss = 0.5274868540048354, val = 0.577581229395796\n",
      "0.576078467706\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    test_nn_with_columns(train_X, train_y, all_columns + [x for x in new_columns_generated if x.startswith('layer1_pca')])\n",
    "# 0.5759712475000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50: loss = 0.5907334422552457, val = 0.5967496885223779\n",
      "epoch 100: loss = 0.5670951068491515, val = 0.5832223275778726\n",
      "epoch 150: loss = 0.5500290452880038, val = 0.5784954736406286\n",
      "epoch 200: loss = 0.5379367678956557, val = 0.5774372410476643\n",
      "0.576675237958\n",
      "epoch 50: loss = 0.5864287301195095, val = 0.5981343093770968\n",
      "epoch 100: loss = 0.5646792716825905, val = 0.5833248486007162\n",
      "epoch 150: loss = 0.5471640735463668, val = 0.5787726602343304\n",
      "epoch 200: loss = 0.5385090508343509, val = 0.5780075173417412\n",
      "epoch 250: loss = 0.5280735885932056, val = 0.5783458846298295\n",
      "0.576167545162\n",
      "epoch 50: loss = 0.5934461387834576, val = 0.5984918959215368\n",
      "epoch 100: loss = 0.5671759191289574, val = 0.5851840766907357\n",
      "epoch 150: loss = 0.547754690401325, val = 0.5799696629525468\n",
      "epoch 200: loss = 0.5397729572526071, val = 0.5780265740259171\n",
      "epoch 250: loss = 0.5292247672447375, val = 0.5786475343842583\n",
      "0.57731011351\n",
      "epoch 50: loss = 0.5899822293452791, val = 0.5985474366870416\n",
      "epoch 100: loss = 0.5642400415672723, val = 0.5852180404395643\n",
      "epoch 150: loss = 0.5502896198720351, val = 0.5798044550996639\n",
      "epoch 200: loss = 0.536769970873866, val = 0.5794782536616103\n",
      "epoch 250: loss = 0.5310205351226158, val = 0.5792107892531131\n",
      "0.578748957752\n",
      "epoch 50: loss = 0.5923238047268039, val = 0.597568945060346\n",
      "epoch 100: loss = 0.5651416634891075, val = 0.5838677708363297\n",
      "epoch 150: loss = 0.5508101858343974, val = 0.5820000703938875\n",
      "epoch 200: loss = 0.5374015237750178, val = 0.5811041919449621\n",
      "epoch 250: loss = 0.5312220110507802, val = 0.580220491462172\n",
      "0.578661298973\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    test_nn_with_columns(train_X, train_y, all_columns + [x for x in new_columns_generated if x.startswith('layer1_knn')])\n",
    "# 0.577724697"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50: loss = 0.5682796702918547, val = 0.5714792323027462\n",
      "epoch 100: loss = 0.5511596248003912, val = 0.5693363437758483\n",
      "epoch 150: loss = 0.5404633747485917, val = 0.569118391474249\n",
      "0.567836367203\n",
      "epoch 50: loss = 0.5657308571257622, val = 0.5723934772818879\n",
      "epoch 100: loss = 0.549216198268699, val = 0.5686539264103256\n",
      "epoch 150: loss = 0.5365811970636242, val = 0.5692361380779911\n",
      "0.567620550835\n",
      "epoch 50: loss = 0.5682518029094431, val = 0.5728822978626177\n",
      "epoch 100: loss = 0.5504466508836087, val = 0.5691213133385742\n",
      "0.568624672508\n",
      "epoch 50: loss = 0.5668625585835892, val = 0.5729637192759991\n",
      "epoch 100: loss = 0.5491214824913193, val = 0.569128129128255\n",
      "epoch 150: loss = 0.5370663559829074, val = 0.5686327860563689\n",
      "0.567867505089\n",
      "epoch 50: loss = 0.567294668367399, val = 0.5739266412399986\n",
      "epoch 100: loss = 0.5500730488976918, val = 0.5695023835236761\n",
      "epoch 150: loss = 0.5388992663967865, val = 0.5699179627594289\n",
      "0.56807929753\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    test_nn_with_columns(train_X, train_y, all_columns + [x for x in new_columns_generated if x.startswith('layer1_rf')])\n",
    "# 0.56814564182608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50: loss = 0.5929667140165115, val = 0.5996124723738684\n",
      "epoch 100: loss = 0.5663412545858033, val = 0.5845144609999939\n",
      "epoch 150: loss = 0.5503417076213359, val = 0.5779046327295982\n",
      "0.577296035055\n",
      "epoch 50: loss = 0.5929926436956255, val = 0.5991310493005774\n",
      "epoch 100: loss = 0.5687427557113137, val = 0.5856202454906284\n",
      "epoch 150: loss = 0.54791703039381, val = 0.5795868810735465\n",
      "epoch 200: loss = 0.5395159363205573, val = 0.5790985688859436\n",
      "0.577202777553\n",
      "epoch 50: loss = 0.5951749289796515, val = 0.6005283730848234\n",
      "epoch 100: loss = 0.5687148210929177, val = 0.5866093116615093\n",
      "epoch 150: loss = 0.5505405192730872, val = 0.5808669920194901\n",
      "epoch 200: loss = 0.5422069537309179, val = 0.5800369410697249\n",
      "0.578376408242\n",
      "epoch 50: loss = 0.593097744737741, val = 0.6007689756345355\n",
      "epoch 100: loss = 0.567927184957491, val = 0.5856907262004433\n",
      "epoch 150: loss = 0.5510727192959162, val = 0.5802431579520432\n",
      "epoch 200: loss = 0.540886766038271, val = 0.5797369761199537\n",
      "0.578568421474\n",
      "epoch 50: loss = 0.5887795204525476, val = 0.5976571049000768\n",
      "epoch 100: loss = 0.5660767066534769, val = 0.5857172937172123\n",
      "epoch 150: loss = 0.5483783360154199, val = 0.580042340666959\n",
      "epoch 200: loss = 0.5390160154705658, val = 0.5790605469100922\n",
      "epoch 250: loss = 0.530357936483448, val = 0.579291543498343\n",
      "0.577676950171\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    test_nn_with_columns(train_X, train_y, all_columns + [x for x in new_columns_generated if x.startswith('layer1_k_means')])\n",
    "# 0.5784255874603335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50: loss = 0.593501963488266, val = 0.6007780395212697\n",
      "epoch 100: loss = 0.568974771543177, val = 0.5851150762515007\n",
      "epoch 150: loss = 0.5527953938210926, val = 0.5792887583797688\n",
      "epoch 200: loss = 0.5402982440046085, val = 0.5776843522912506\n",
      "0.576746761903\n",
      "epoch 50: loss = 0.5894670833540084, val = 0.5975216648763767\n",
      "epoch 100: loss = 0.5641740372123354, val = 0.5833631754984324\n",
      "epoch 150: loss = 0.5490681999002947, val = 0.5792636525826123\n",
      "epoch 200: loss = 0.5427933462053101, val = 0.5776859821579912\n",
      "epoch 250: loss = 0.5346804329098969, val = 0.5783245299445653\n",
      "0.577357405282\n",
      "epoch 50: loss = 0.5930712994077562, val = 0.6002991233213656\n",
      "epoch 100: loss = 0.5705091002089616, val = 0.5842558635577794\n",
      "epoch 150: loss = 0.5533849896675198, val = 0.5807215348308796\n",
      "epoch 200: loss = 0.5392204656743691, val = 0.5780524985777499\n",
      "epoch 250: loss = 0.5313960316019248, val = 0.5781242910438621\n",
      "0.577002162568\n",
      "epoch 50: loss = 0.592848019352053, val = 0.5984043470463233\n",
      "epoch 100: loss = 0.5685832772474206, val = 0.5838604958908815\n",
      "epoch 150: loss = 0.5519678361084651, val = 0.5791703257767109\n",
      "epoch 200: loss = 0.5426742436820519, val = 0.5785901439037252\n",
      "epoch 250: loss = 0.5339669727548669, val = 0.5782302164977327\n",
      "0.576641629729\n",
      "epoch 50: loss = 0.596250282126784, val = 0.6002803701484082\n",
      "epoch 100: loss = 0.5688310495495776, val = 0.5852726109315016\n",
      "epoch 150: loss = 0.5559129860056286, val = 0.5807252391595511\n",
      "epoch 200: loss = 0.5418542105332762, val = 0.5783889397246889\n",
      "epoch 250: loss = 0.5341640600571597, val = 0.5781387986346879\n",
      "0.57698450397\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    test_nn_with_columns(train_X, train_y, all_columns + [x for x in new_columns_generated if x.startswith('layer1_ridge')])\n",
    "# 0.5784149550699805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "class FakeDMatrix:\n",
    "    def __init__(self, data, labels=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.num = len(data)\n",
    "\n",
    "    def num_row(self):\n",
    "        return self.num\n",
    "\n",
    "    def slice(self, rindex):\n",
    "        indices = np.zeros(self.num, dtype=np.bool)\n",
    "        for index in rindex:\n",
    "            indices[index] = True\n",
    "        return FakeDMatrix(data=self.data[indices], labels=self.labels[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09102+3.71573e-05\ttest-mlogloss:1.09122+1.91369e-05\n",
      "[200]\ttrain-mlogloss:0.573924+0.00207436\ttest-mlogloss:0.607135+0.00328306\n",
      "[400]\ttrain-mlogloss:0.499542+0.00258495\ttest-mlogloss:0.55791+0.00442858\n",
      "[600]\ttrain-mlogloss:0.466972+0.00312354\ttest-mlogloss:0.547912+0.00461182\n",
      "[800]\ttrain-mlogloss:0.442694+0.0031566\ttest-mlogloss:0.544045+0.00467582\n",
      "[1000]\ttrain-mlogloss:0.422039+0.00339214\ttest-mlogloss:0.542069+0.00467361\n",
      "[1200]\ttrain-mlogloss:0.402974+0.00281161\ttest-mlogloss:0.540841+0.00466194\n",
      "[1400]\ttrain-mlogloss:0.385328+0.0026261\ttest-mlogloss:0.540158+0.00472587\n",
      "[1600]\ttrain-mlogloss:0.368956+0.00246329\ttest-mlogloss:0.539705+0.00470666\n",
      "[1800]\ttrain-mlogloss:0.353754+0.00244792\ttest-mlogloss:0.539548+0.00468775\n",
      "0.539502\n",
      "1863\n"
     ]
    }
   ],
   "source": [
    "SEED = 777\n",
    "NFOLDS = 3\n",
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.8,\n",
    "    'subsample':.8,\n",
    "    'seed':0,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(data=train_X, label=train_y)\n",
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200)\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "print (bst['test-mlogloss-mean'][best_rounds])\n",
    "print (best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(xtr):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(500, input_dim=xtr.shape[1], init = 'he_normal', activation='sigmoid'))\n",
    "    #model.add(Dense(64, input_dim=64, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(PReLU())\n",
    "\n",
    "    model.add(Dense(100, init = 'he_normal', activation='sigmoid'))\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(PReLU())\n",
    "\n",
    "    model.add(Dense(3, init = 'he_normal', activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_nn_with_columns(X, y, columns_to_include):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    ntrain = int(len(train_X) * 0.75)\n",
    "\n",
    "    xtr = X_scaled[columns_to_include][:ntrain]\n",
    "    ytr = y[:ntrain].values\n",
    "    ytr_one_hot = np.eye(3)[ytr]\n",
    "    xte = X_scaled[columns_to_include][ntrain:]\n",
    "    yte = y[ntrain:].values\n",
    "    yte_one_hot = np.eye(3)[yte]\n",
    "\n",
    "    filepath = 'data/nn_weights'\n",
    "\n",
    "    logger = NBatchLogger()\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=40, verbose=0)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "    model = create_model(xtr)\n",
    "    model.fit(xtr.values, ytr_one_hot, nb_epoch = 1200, batch_size=1000, verbose = 0, validation_data=[xte.values, yte_one_hot], callbacks=[logger, early_stop, checkpoint])\n",
    "    model = create_model(xtr)\n",
    "    model.load_weights(filepath)\n",
    "    print (log_loss(yte_one_hot, model.predict(xte.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new_vects = ['tf_idf_' + x[7:] for x in sparse_cols]\n",
    "#sparse = processed_train[sparse_cols]\n",
    "#tf_idfs = TfidfTransformer().fit_transform(sparse)\n",
    "#tf_idfs_df = pd.DataFrame(tf_idfs.toarray(), columns=new_vects)\n",
    "#processed_train = processed_train.join(tf_idfs_df, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Lasso Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "las = Lasso()\n",
    "enet = ElasticNet()\n",
    "las.fit(xte, yte)\n",
    "enet.fit(xte, yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "10.478087214\n"
     ]
    }
   ],
   "source": [
    "print (las.score(xte, yte))\n",
    "predictions = las.predict(xte)\n",
    "print (log_loss(yte_one_hot, np.eye(3)[np.round(predictions).astype(int)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "10.478087214\n"
     ]
    }
   ],
   "source": [
    "print (enet.score(xte, yte))\n",
    "predictions = enet.predict(xte)\n",
    "print (log_loss(yte_one_hot, np.eye(3)[np.round(predictions).astype(int)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for coef, column in zip(enet.coef_, train_X.columns):\n",
    "    if coef != 0:\n",
    "        print (column, \": \", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for coef, column in zip(las.coef_, train_X.columns):\n",
    "    if coef != 0:\n",
    "        print (column, \": \", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5843451108603969 (test loss 0.3912286072900344)\n"
     ]
    }
   ],
   "source": [
    "test = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "\n",
    "test.fit(xtr, ytr)\n",
    "            \n",
    "predictions = test.predict_proba(xtr)\n",
    "et_predictions = test.predict_proba(xte)\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, et_predictions), log_loss(ytr_one_hot, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5841850290743527 (test loss 0.3945679429419886)\n"
     ]
    }
   ],
   "source": [
    "test = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "\n",
    "test.fit(xtr, ytr)\n",
    "            \n",
    "predictions = test.predict_proba(xtr)\n",
    "et_ent_predictions = test.predict_proba(xte)\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, et_ent_predictions), log_loss(ytr_one_hot, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5693372215730407 (test loss 0.3780482702094549)\n"
     ]
    }
   ],
   "source": [
    "test = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "\n",
    "test.fit(xtr, ytr)\n",
    "            \n",
    "predictions = test.predict_proba(xtr)\n",
    "rf_predictions = test.predict_proba(xte)\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, rf_predictions), log_loss(ytr_one_hot, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5668175898671493 (test loss 0.3773447326368733)\n"
     ]
    }
   ],
   "source": [
    "test = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "\n",
    "test.fit(xtr, ytr)\n",
    "            \n",
    "predictions = test.predict_proba(xtr)\n",
    "rf_ent_predictions = test.predict_proba(xte)\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, rf_ent_predictions), log_loss(ytr_one_hot, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56688904952650898"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mixed = (0.01 * et_predictions + 0.01 * et_ent_predictions + 0.49 * rf_predictions + 0.49 * rf_ent_predictions)\n",
    "log_loss(yte_one_hot, pred_mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.076257047822155 (test loss 0.23129250627049086)\n",
      "CPU times: user 3min 15s, sys: 1.48 s, total: 3min 16s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=2)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions = test.predict_proba(xte[non_sparse_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.997161309463761 (test loss 0.4189513500335195)\n",
      "CPU times: user 3min 36s, sys: 1.02 s, total: 3min 37s\n",
      "Wall time: 3min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=5)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions_2 = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions_2 = test.predict_proba(xte[non_sparse_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5474359459916005 (test loss 0.4994428601539661)\n",
      "CPU times: user 3min 48s, sys: 839 ms, total: 3min 49s\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=10)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions_3 = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions_3 = test.predict_proba(xte[non_sparse_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6344317626130038 (test loss 0.6195293255389913)\n",
      "CPU times: user 4min 34s, sys: 1.19 s, total: 4min 35s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=200)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions_6 = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions_6 = test.predict_proba(xte[non_sparse_cols])\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, rf_ent_predictions_6), log_loss(ytr_one_hot, predictions_6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feat, importance in zip(processed_train[cols].columns, clf.feature_importances_):\n",
    "    if not np.isnan(importance) and importance > 0:\n",
    "        print (feat, \": \", importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn:  0.563593691555\n",
      "ab:  0.566889049527\n",
      "combo:  0.55827286106\n"
     ]
    }
   ],
   "source": [
    "print (\"nn: \", log_loss(yte_one_hot, nn_predictions))\n",
    "print (\"ab: \", log_loss(yte_one_hot, pred_mixed))\n",
    "frac_nn = 0.2\n",
    "averaged_preds = (nn_predictions * frac_nn) + (pred_mixed * (1 - frac_nn))\n",
    "print (\"combo: \", log_loss(yte_one_hot, averaged_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = pd.read_json('data/train_leaky.json')\n",
    "train_y = pd.read_json('data/train_interest.json')[0]\n",
    "scaler = StandardScaler().fit(train_X)\n",
    "train_X = pd.DataFrame(scaler.transform(train_X), columns=columns)\n",
    "test_X = pd.read_json('data/test_leaky.json')\n",
    "test_X = pd.DataFrame(scaler.transform(test_X), columns=columns)\n",
    "listing_id = pd.read_json('data/test_ids.json').values\n",
    "preds = pd.read_csv('data/my_best_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = pd.read_csv('data/my_best_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_clf = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='gini',\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "et_clf.fit(train_X, train_y)          \n",
    "et_predictions = et_clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_clf_ent = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "et_clf_ent.fit(train_X, train_y)          \n",
    "et_ent_predictions = et_clf_ent.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='gini',\n",
    "    min_samples_split=20)\n",
    "rf_clf.fit(train_X, train_y)          \n",
    "rf_predictions = rf_clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_clf_ent = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    min_samples_split=20)\n",
    "rf_clf_ent.fit(train_X, train_y)          \n",
    "rf_ent_predictions = rf_clf_ent.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_predictions = model.predict(test_X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_predictions = preds[['high', 'medium', 'low']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_preds = (0.8 * xgb_predictions) + (0.15 * nn_predictions) + (0.05 * et_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_preds_df = pd.DataFrame(new_preds, columns=['high', 'medium', 'low'])\n",
    "new_preds_df['listing_id'] = preds['listing_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_preds_df.to_csv('data/combined_preds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
