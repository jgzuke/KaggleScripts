{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgzuke/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.metrics import log_loss\n",
    "import string\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from scipy.stats import boxcox\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.callbacks import Callback\n",
    "import xgboost as xgb\n",
    "\n",
    "class NBatchLogger(Callback):\n",
    "    def __init__(self):\n",
    "        self.seen = 0\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.seen += 1\n",
    "        if self.seen % 500 == 0:\n",
    "            print('epoch {}: loss = {}, val = {}'.format(self.seen, logs['loss'], logs['val_loss'])) \n",
    "            \n",
    "class FakeDMatrix:\n",
    "    def __init__(self, data, labels=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.num = len(data)\n",
    "\n",
    "    def num_row(self):\n",
    "        return self.num\n",
    "\n",
    "    def slice(self, rindex):\n",
    "        indices = np.zeros(self.num, dtype=np.bool)\n",
    "        for index in rindex:\n",
    "            indices[index] = True\n",
    "        return FakeDMatrix(data=self.data[indices], labels=self.labels[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = pd.read_json('data/train_with_leaky_cv.json')\n",
    "train_y = pd.read_json('data/train_interest.json')[0]\n",
    "columns = train_X.columns\n",
    "train_X_scaled = pd.DataFrame(StandardScaler().fit_transform(train_X), columns=columns)\n",
    "#train_X_with_layer_1 = pd.read_json('data/train_scaled_with_leaky_and_added_features_cv.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a bunch of features to train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BoroCode',\n",
       " 'Day',\n",
       " 'Month',\n",
       " 'NTACode',\n",
       " 'Wday',\n",
       " 'Yday',\n",
       " 'Zero_building_id',\n",
       " 'Zero_manager_id',\n",
       " 'address_high_frac',\n",
       " 'address_low_frac',\n",
       " 'address_med_frac',\n",
       " 'address_worths',\n",
       " 'avenue',\n",
       " 'bathrooms',\n",
       " 'bc_price',\n",
       " 'bedrooms',\n",
       " 'boro_high_frac',\n",
       " 'boro_low_frac',\n",
       " 'boro_med_frac',\n",
       " 'boro_worths',\n",
       " 'building_high_frac',\n",
       " 'building_id',\n",
       " 'building_low_frac',\n",
       " 'building_med_frac',\n",
       " 'building_worths',\n",
       " 'desc_letters_count',\n",
       " 'desc_words_count',\n",
       " 'desc_words_length',\n",
       " 'display_address',\n",
       " 'east',\n",
       " 'features_count',\n",
       " 'hour',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'manager_high_frac',\n",
       " 'manager_id',\n",
       " 'manager_low_frac',\n",
       " 'manager_med_frac',\n",
       " 'manager_worths',\n",
       " 'north',\n",
       " 'nta_high_frac',\n",
       " 'nta_low_frac',\n",
       " 'nta_med_frac',\n",
       " 'nta_worths',\n",
       " 'other_address',\n",
       " 'photos_count',\n",
       " 'price_by_BoroCode',\n",
       " 'price_by_NTACode',\n",
       " 'price_by_area',\n",
       " 'price_by_building_id',\n",
       " 'price_by_display_address',\n",
       " 'price_by_manager_id',\n",
       " 'price_per_bathroom',\n",
       " 'price_per_bed_and_bath',\n",
       " 'price_per_bedroom',\n",
       " 'real_minus_expected_price',\n",
       " 'real_over_expected_price',\n",
       " 'south',\n",
       " 'sparse_24',\n",
       " 'sparse_2fullbaths',\n",
       " 'sparse_actualapt',\n",
       " 'sparse_airconditioning',\n",
       " 'sparse_allutilitiesincluded',\n",
       " 'sparse_assignedparkingspace',\n",
       " 'sparse_attendedlobby',\n",
       " 'sparse_backyard',\n",
       " 'sparse_balconies',\n",
       " 'sparse_balcony',\n",
       " 'sparse_basementstorage',\n",
       " 'sparse_bikeroom',\n",
       " 'sparse_bikestorage',\n",
       " 'sparse_billiardsroom',\n",
       " 'sparse_brownstone',\n",
       " 'sparse_buildingcommonoutdoorspace',\n",
       " 'sparse_businesscenter',\n",
       " 'sparse_cable',\n",
       " 'sparse_catsallowed',\n",
       " 'sparse_centrala',\n",
       " 'sparse_centralac',\n",
       " 'sparse_centralair',\n",
       " 'sparse_chef',\n",
       " 'sparse_chefskitchen',\n",
       " 'sparse_children',\n",
       " 'sparse_childrensplayroom',\n",
       " 'sparse_cityview',\n",
       " 'sparse_commonbackyard',\n",
       " 'sparse_commongarden',\n",
       " 'sparse_commonoutdoorspace',\n",
       " 'sparse_commonparking',\n",
       " 'sparse_commonroofdeck',\n",
       " 'sparse_commonterrace',\n",
       " 'sparse_communityrecreationfacilities',\n",
       " 'sparse_concierge',\n",
       " 'sparse_conciergeservice',\n",
       " 'sparse_condofinishes',\n",
       " 'sparse_cook',\n",
       " 'sparse_courtyard',\n",
       " 'sparse_deck',\n",
       " 'sparse_decorativefireplace',\n",
       " 'sparse_diningroom',\n",
       " 'sparse_dishwasher',\n",
       " 'sparse_dogsallowed',\n",
       " 'sparse_doorman',\n",
       " 'sparse_drycleaningservice',\n",
       " 'sparse_dryer',\n",
       " 'sparse_dryerinbuilding',\n",
       " 'sparse_dryerinunit',\n",
       " 'sparse_duplex',\n",
       " 'sparse_eatinkitchen',\n",
       " 'sparse_elev',\n",
       " 'sparse_elevator',\n",
       " 'sparse_elevbldg',\n",
       " 'sparse_exclusive',\n",
       " 'sparse_exposedbrick',\n",
       " 'sparse_fireplace',\n",
       " 'sparse_fireplaces',\n",
       " 'sparse_fitness',\n",
       " 'sparse_fitnesscenter',\n",
       " 'sparse_flex2',\n",
       " 'sparse_flex3',\n",
       " 'sparse_ftdoorman',\n",
       " 'sparse_fullservicegarage',\n",
       " 'sparse_fulltimedoorman',\n",
       " 'sparse_furnished',\n",
       " 'sparse_garage',\n",
       " 'sparse_garden',\n",
       " 'sparse_gourmetkitchen',\n",
       " 'sparse_granitecountertops',\n",
       " 'sparse_granitekitchen',\n",
       " 'sparse_greenbuilding',\n",
       " 'sparse_guarantorsaccepted',\n",
       " 'sparse_gutrenovated',\n",
       " 'sparse_gym',\n",
       " 'sparse_gyminbuilding',\n",
       " 'sparse_hardwood',\n",
       " 'sparse_hardwoodfloors',\n",
       " 'sparse_healthclub',\n",
       " 'sparse_highceiling',\n",
       " 'sparse_highceilings',\n",
       " 'sparse_highrise',\n",
       " 'sparse_highspeedinternet',\n",
       " 'sparse_hirise',\n",
       " 'sparse_housekeeping',\n",
       " 'sparse_indoorpool',\n",
       " 'sparse_inunitwasher',\n",
       " 'sparse_largelivingroom',\n",
       " 'sparse_laundry',\n",
       " 'sparse_laundryinbuilding',\n",
       " 'sparse_laundryinunit',\n",
       " 'sparse_laundryonfloor',\n",
       " 'sparse_laundryroom',\n",
       " 'sparse_light',\n",
       " 'sparse_live',\n",
       " 'sparse_liveinsuper',\n",
       " 'sparse_liveinsuperintendent',\n",
       " 'sparse_lndrybldg',\n",
       " 'sparse_loft',\n",
       " 'sparse_lounge',\n",
       " 'sparse_loungeroom',\n",
       " 'sparse_lowrise',\n",
       " 'sparse_luxurybuilding',\n",
       " 'sparse_mailroom',\n",
       " 'sparse_marblebath',\n",
       " 'sparse_marblebathroom',\n",
       " 'sparse_microwave',\n",
       " 'sparse_midrise',\n",
       " 'sparse_mrcleanapproved',\n",
       " 'sparse_multilevel',\n",
       " 'sparse_new',\n",
       " 'sparse_newconstruction',\n",
       " 'sparse_newlyrenovated',\n",
       " 'sparse_nofee',\n",
       " 'sparse_nopets',\n",
       " 'sparse_nursery',\n",
       " 'sparse_onsitegarage',\n",
       " 'sparse_onsitelaundry',\n",
       " 'sparse_onsiteparking',\n",
       " 'sparse_onsiteparkingavailable',\n",
       " 'sparse_onsiteparkinglot',\n",
       " 'sparse_onsitesuper',\n",
       " 'sparse_ornateprewardetails',\n",
       " 'sparse_outdoorareas',\n",
       " 'sparse_outdoorentertainmentspace',\n",
       " 'sparse_outdoorpool',\n",
       " 'sparse_outdoorspace',\n",
       " 'sparse_packageroom',\n",
       " 'sparse_parking',\n",
       " 'sparse_parkingspace',\n",
       " 'sparse_patio',\n",
       " 'sparse_penthouse',\n",
       " 'sparse_petfriendly',\n",
       " 'sparse_pets',\n",
       " 'sparse_petsallowed',\n",
       " 'sparse_petsok',\n",
       " 'sparse_petsonapproval',\n",
       " 'sparse_photos',\n",
       " 'sparse_playroom',\n",
       " 'sparse_pool',\n",
       " 'sparse_postwar',\n",
       " 'sparse_prewar',\n",
       " 'sparse_privatebackyard',\n",
       " 'sparse_privatebalcony',\n",
       " 'sparse_privatedeck',\n",
       " 'sparse_privatelaundryroomoneveryfloor',\n",
       " 'sparse_privateoutdoorspace',\n",
       " 'sparse_privateparking',\n",
       " 'sparse_privateroofdeck',\n",
       " 'sparse_privateterrace',\n",
       " 'sparse_publicoutdoor',\n",
       " 'sparse_reducedfee',\n",
       " 'sparse_renovated',\n",
       " 'sparse_residentsgarden',\n",
       " 'sparse_residentslounge',\n",
       " 'sparse_roofaccess',\n",
       " 'sparse_roofdeck',\n",
       " 'sparse_rooftopdeck',\n",
       " 'sparse_rooftopterrace',\n",
       " 'sparse_roomyclosets',\n",
       " 'sparse_satellitetv',\n",
       " 'sparse_sauna',\n",
       " 'sparse_screeningroom',\n",
       " 'sparse_sharedbackyard',\n",
       " 'sparse_sharesok',\n",
       " 'sparse_shorttermallowed',\n",
       " 'sparse_simplex',\n",
       " 'sparse_skitchen',\n",
       " 'sparse_skylight',\n",
       " 'sparse_splayroom',\n",
       " 'sparse_sskitchen',\n",
       " 'sparse_stainlesssteelappliances',\n",
       " 'sparse_stepstothepark',\n",
       " 'sparse_storage',\n",
       " 'sparse_storagefacilitiesavailable',\n",
       " 'sparse_storageroom',\n",
       " 'sparse_sublet',\n",
       " 'sparse_subway',\n",
       " 'sparse_sundeck',\n",
       " 'sparse_swimmingpool',\n",
       " 'sparse_tenantlounge',\n",
       " 'sparse_terrace',\n",
       " 'sparse_terraces',\n",
       " 'sparse_tonsofnaturallight',\n",
       " 'sparse_valet',\n",
       " 'sparse_valetparking',\n",
       " 'sparse_valetservices',\n",
       " 'sparse_valetservicesincludingdrycleaning',\n",
       " 'sparse_videointercom',\n",
       " 'sparse_view',\n",
       " 'sparse_virtualdoorman',\n",
       " 'sparse_walkincloset',\n",
       " 'sparse_wallsofwindows',\n",
       " 'sparse_washer',\n",
       " 'sparse_washerinunit',\n",
       " 'sparse_wheelchairaccess',\n",
       " 'sparse_wheelchairramp',\n",
       " 'sparse_wifi',\n",
       " 'sparse_wifiaccess',\n",
       " 'sparse_work',\n",
       " 'street',\n",
       " 'west',\n",
       " 'worth_by_BoroCode',\n",
       " 'worth_by_NTACode',\n",
       " 'worth_by_building_id',\n",
       " 'worth_by_display_address',\n",
       " 'worth_by_manager_id']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_X_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntrain = int(len(train_X) * 0.75)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "xtr = train_X[:ntrain]\n",
    "xtr = pd.DataFrame(scaler.fit_transform(xtr), columns=xtr.columns)\n",
    "ytr = train_y[:ntrain].values\n",
    "ytr_one_hot = np.eye(3)[ytr]\n",
    "xte = train_X[ntrain:]\n",
    "xte = pd.DataFrame(scaler.transform(xte), columns=xte.columns)\n",
    "yte = train_y[ntrain:].values\n",
    "yte_one_hot = np.eye(3)[yte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NFOLDS = 5\n",
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.6,\n",
    "    'subsample':.6,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.0915+1.37026e-05\ttest-mlogloss:1.09172+5.17301e-05\n",
      "[200]\ttrain-mlogloss:0.593105+0.00097267\ttest-mlogloss:0.625639+0.00312624\n",
      "[400]\ttrain-mlogloss:0.510146+0.00104157\ttest-mlogloss:0.566794+0.00367694\n",
      "[600]\ttrain-mlogloss:0.47098+0.000857232\ttest-mlogloss:0.550018+0.00341384\n",
      "[800]\ttrain-mlogloss:0.442976+0.000962218\ttest-mlogloss:0.542362+0.00310859\n",
      "[1000]\ttrain-mlogloss:0.42006+0.000760993\ttest-mlogloss:0.53793+0.00297711\n",
      "[1200]\ttrain-mlogloss:0.399763+0.000495187\ttest-mlogloss:0.535118+0.00287364\n",
      "[1400]\ttrain-mlogloss:0.381234+0.000621526\ttest-mlogloss:0.533206+0.0028097\n",
      "[1600]\ttrain-mlogloss:0.363973+0.000679936\ttest-mlogloss:0.531899+0.00255041\n",
      "[1800]\ttrain-mlogloss:0.347758+0.000647169\ttest-mlogloss:0.530938+0.00247888\n",
      "[2000]\ttrain-mlogloss:0.332424+0.000563903\ttest-mlogloss:0.530287+0.00243055\n",
      "[2200]\ttrain-mlogloss:0.318075+0.000614961\ttest-mlogloss:0.529914+0.00230364\n",
      "0.529903\n",
      "train score:  0.343920598363\n",
      "test score:  0.537693293521\n",
      "CPU times: user 3h 47min 45s, sys: 8min 20s, total: 3h 56min 5s\n",
      "Wall time: 1h 13min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cols = [col for col in list(train_X.columns) if not col.endswith('worths')]\n",
    "dtrain = xgb.DMatrix(data=xtr[cols], label=ytr)\n",
    "dtest = xgb.DMatrix(data=xte[cols])\n",
    "\n",
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200)\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "print (bst['test-mlogloss-mean'][best_rounds])\n",
    "\n",
    "bst = xgb.train(params, dtrain, best_rounds)\n",
    "print('train score: ', log_loss(ytr_one_hot, bst.predict(dtrain)))\n",
    "print('test score: ', log_loss(yte_one_hot, bst.predict(dtest)))\n",
    "# 0.531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09144+1.57404e-05\ttest-mlogloss:1.09164+4.05788e-05\n",
      "[200]\ttrain-mlogloss:0.595084+0.0012245\ttest-mlogloss:0.627589+0.0036066\n",
      "[400]\ttrain-mlogloss:0.514481+0.00133087\ttest-mlogloss:0.57157+0.00447855\n",
      "[600]\ttrain-mlogloss:0.476159+0.00136191\ttest-mlogloss:0.556453+0.00456728\n",
      "[800]\ttrain-mlogloss:0.447741+0.00149133\ttest-mlogloss:0.549709+0.00430032\n",
      "[1000]\ttrain-mlogloss:0.424094+0.00160832\ttest-mlogloss:0.545748+0.00410376\n",
      "[1200]\ttrain-mlogloss:0.403252+0.00158531\ttest-mlogloss:0.543422+0.00385456\n",
      "[1400]\ttrain-mlogloss:0.384322+0.00152677\ttest-mlogloss:0.541796+0.0035792\n",
      "[1600]\ttrain-mlogloss:0.366528+0.00151377\ttest-mlogloss:0.540824+0.00342349\n",
      "[1800]\ttrain-mlogloss:0.349964+0.00154613\ttest-mlogloss:0.540202+0.00320404\n",
      "[2000]\ttrain-mlogloss:0.334427+0.0016487\ttest-mlogloss:0.539839+0.00306342\n",
      "0.5397868\n",
      "train score:  0.356810162847\n",
      "test score:  0.548521112096\n",
      "CPU times: user 1h 15min 49s, sys: 7min 28s, total: 1h 23min 17s\n",
      "Wall time: 29min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cols = [col for col in list(train_X.columns) if not col.startswith('sparse_')]\n",
    "dtrain = xgb.DMatrix(data=xtr[cols], label=ytr)\n",
    "dtest = xgb.DMatrix(data=xte[cols])\n",
    "\n",
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200)\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "print (bst['test-mlogloss-mean'][best_rounds])\n",
    "\n",
    "bst = xgb.train(params, dtrain, best_rounds)\n",
    "print('train score: ', log_loss(ytr_one_hot, bst.predict(dtrain)))\n",
    "print('test score: ', log_loss(yte_one_hot, bst.predict(dtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09156+1.58367e-05\ttest-mlogloss:1.09176+4.50671e-05\n",
      "[200]\ttrain-mlogloss:0.603364+0.00101354\ttest-mlogloss:0.63414+0.00377109\n",
      "[400]\ttrain-mlogloss:0.522066+0.00100241\ttest-mlogloss:0.576203+0.00448577\n",
      "[600]\ttrain-mlogloss:0.482695+0.000917929\ttest-mlogloss:0.558561+0.00427423\n",
      "[800]\ttrain-mlogloss:0.453882+0.00113939\ttest-mlogloss:0.5498+0.00405268\n",
      "[1000]\ttrain-mlogloss:0.430137+0.000853047\ttest-mlogloss:0.544584+0.00384638\n",
      "[1200]\ttrain-mlogloss:0.40918+0.000844477\ttest-mlogloss:0.541058+0.00357907\n",
      "[1400]\ttrain-mlogloss:0.39033+0.00069565\ttest-mlogloss:0.538678+0.00335179\n",
      "[1600]\ttrain-mlogloss:0.372895+0.000831401\ttest-mlogloss:0.536926+0.0033013\n",
      "[1800]\ttrain-mlogloss:0.356532+0.000868897\ttest-mlogloss:0.53575+0.00312374\n",
      "[2000]\ttrain-mlogloss:0.3412+0.00083446\ttest-mlogloss:0.534905+0.00309618\n",
      "[2200]\ttrain-mlogloss:0.326556+0.000780556\ttest-mlogloss:0.534452+0.00305813\n",
      "[2400]\ttrain-mlogloss:0.312724+0.000828834\ttest-mlogloss:0.5343+0.00295067\n",
      "0.5343002\n",
      "train score:  0.340156854749\n",
      "test score:  0.544540325092\n",
      "CPU times: user 3h 53min 20s, sys: 8min 50s, total: 4h 2min 11s\n",
      "Wall time: 1d 4h 18min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cols = [col for col in list(train_X.columns) if (not col.startswith('worth_')) and (not col.startswith('price_'))]\n",
    "dtrain = xgb.DMatrix(data=xtr[cols], label=ytr)\n",
    "dtest = xgb.DMatrix(data=xte[cols])\n",
    "\n",
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200)\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "print (bst['test-mlogloss-mean'][best_rounds])\n",
    "\n",
    "bst = xgb.train(params, dtrain, best_rounds)\n",
    "print('train score: ', log_loss(ytr_one_hot, bst.predict(dtrain)))\n",
    "print('test score: ', log_loss(yte_one_hot, bst.predict(dtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08821+5.87524e-05\ttest-mlogloss:1.08825+3.62844e-05\n",
      "[200]\ttrain-mlogloss:0.68701+0.000749359\ttest-mlogloss:0.694804+0.00382705\n",
      "[400]\ttrain-mlogloss:0.670175+0.000888663\ttest-mlogloss:0.683931+0.00381625\n",
      "[600]\ttrain-mlogloss:0.662602+0.000920479\ttest-mlogloss:0.680848+0.00360242\n",
      "[800]\ttrain-mlogloss:0.656657+0.000894507\ttest-mlogloss:0.679044+0.00347421\n",
      "[1000]\ttrain-mlogloss:0.651419+0.000845896\ttest-mlogloss:0.677794+0.00336274\n",
      "[1200]\ttrain-mlogloss:0.646637+0.000825468\ttest-mlogloss:0.676902+0.00327914\n",
      "[1400]\ttrain-mlogloss:0.642055+0.000789888\ttest-mlogloss:0.67624+0.0032678\n",
      "[1600]\ttrain-mlogloss:0.637828+0.000775101\ttest-mlogloss:0.67566+0.00328069\n",
      "[1800]\ttrain-mlogloss:0.63376+0.000789535\ttest-mlogloss:0.675215+0.00324592\n",
      "[2000]\ttrain-mlogloss:0.629968+0.000868979\ttest-mlogloss:0.67492+0.00324037\n",
      "[2200]\ttrain-mlogloss:0.626273+0.000786515\ttest-mlogloss:0.674645+0.00326982\n",
      "[2400]\ttrain-mlogloss:0.622771+0.000792332\ttest-mlogloss:0.674467+0.00333496\n",
      "[2600]\ttrain-mlogloss:0.619334+0.000843996\ttest-mlogloss:0.674384+0.00324989\n",
      "0.674331\n",
      "train score:  0.625463983324\n",
      "test score:  0.673308849908\n",
      "CPU times: user 2h 26min 47s, sys: 5min 8s, total: 2h 31min 55s\n",
      "Wall time: 49min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.6,\n",
    "    'subsample':.6,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'eta':.02,\n",
    "    'colsample_bytree':.7,\n",
    "    'subsample':.7,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1,\n",
    "    \n",
    "    'max_depth':4,\n",
    "    'min_child_weight': 1,\n",
    "}\n",
    "\n",
    "cols = [col for col in list(train_X.columns) if col.startswith('sparse_')] + ['price_per_bedroom']\n",
    "dtrain = xgb.DMatrix(data=xtr[cols], label=ytr)\n",
    "dtest = xgb.DMatrix(data=xte[cols])\n",
    "\n",
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200)\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "print (bst['test-mlogloss-mean'][best_rounds])\n",
    "\n",
    "bst = xgb.train(params, dtrain, best_rounds)\n",
    "print('train score: ', log_loss(ytr_one_hot, bst.predict(dtrain)))\n",
    "print('test score: ', log_loss(yte_one_hot, bst.predict(dtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[2600]\ttrain-mlogloss:0.600032+0.000895735\ttest-mlogloss:0.671792+0.00350823\n",
    "0.6717898\n",
    "train score:  0.609815663253\n",
    "test score:  0.670838713845\n",
    "CPU times: user 3h 4min 57s, sys: 7min 30s, total: 3h 12min 28s\n",
    "Wall time: 1h 1min 48s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.6,\n",
    "    'subsample':.6,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "layer_1_scaler = StandardScaler()\n",
    "train_X_with_layer_1 = pd.read_json('data/train_with_leaky_cv.json')\n",
    "train_y = pd.read_json('data/train_interest.json')[0]\n",
    "columns_minus_layer_1 = train_X_with_layer_1.columns\n",
    "train_X_scaled = pd.DataFrame(layer_1_scaler.fit_transform(train_X_with_layer_1), columns=columns_minus_layer_1)\n",
    "\n",
    "n_components = 3\n",
    "pca_columns = ['layer1_pca_{}'.format(i) for i in range(n_components)]\n",
    "sparse_prediction_cols = ['predict_sparse_{}'.format(category) for category in ['high', 'medium', 'low']]\n",
    "knn_n_closest = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "knn_columns = ['layer1_knn_{}_{}'.format(n_neighbors, category) for category in ['high', 'medium', 'low'] for n_neighbors in knn_n_closest]\n",
    "ridge_columns = ['layer1_ridge_predictions']\n",
    "rf_columns = ['layer1_rf_{}_{}_predictions'.format(criterion, category) for category in ['high', 'medium', 'low'] for criterion in ['gini', 'entropy']]\n",
    "k_means_columns = ['layer1_k_means_{}_{}'.format(n_clusters, index) for n_clusters in [2, 4, 8] for index in range(n_clusters)]\n",
    "\n",
    "all_layer_one_columns = pca_columns + sparse_prediction_cols + knn_columns + ridge_columns + rf_columns + k_means_columns\n",
    "for column in all_layer_one_columns:\n",
    "    train_X_with_layer_1[column] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['layer1_pca_0',\n",
       " 'layer1_pca_1',\n",
       " 'layer1_pca_2',\n",
       " 'predict_sparse_high',\n",
       " 'predict_sparse_medium',\n",
       " 'predict_sparse_low',\n",
       " 'layer1_knn_2_high',\n",
       " 'layer1_knn_4_high',\n",
       " 'layer1_knn_8_high',\n",
       " 'layer1_knn_16_high',\n",
       " 'layer1_knn_32_high',\n",
       " 'layer1_knn_64_high',\n",
       " 'layer1_knn_128_high',\n",
       " 'layer1_knn_256_high',\n",
       " 'layer1_knn_512_high',\n",
       " 'layer1_knn_1024_high',\n",
       " 'layer1_knn_2_medium',\n",
       " 'layer1_knn_4_medium',\n",
       " 'layer1_knn_8_medium',\n",
       " 'layer1_knn_16_medium',\n",
       " 'layer1_knn_32_medium',\n",
       " 'layer1_knn_64_medium',\n",
       " 'layer1_knn_128_medium',\n",
       " 'layer1_knn_256_medium',\n",
       " 'layer1_knn_512_medium',\n",
       " 'layer1_knn_1024_medium',\n",
       " 'layer1_knn_2_low',\n",
       " 'layer1_knn_4_low',\n",
       " 'layer1_knn_8_low',\n",
       " 'layer1_knn_16_low',\n",
       " 'layer1_knn_32_low',\n",
       " 'layer1_knn_64_low',\n",
       " 'layer1_knn_128_low',\n",
       " 'layer1_knn_256_low',\n",
       " 'layer1_knn_512_low',\n",
       " 'layer1_knn_1024_low',\n",
       " 'layer1_ridge_predictions',\n",
       " 'layer1_rf_gini_high_predictions',\n",
       " 'layer1_rf_entropy_high_predictions',\n",
       " 'layer1_rf_gini_medium_predictions',\n",
       " 'layer1_rf_entropy_medium_predictions',\n",
       " 'layer1_rf_gini_low_predictions',\n",
       " 'layer1_rf_entropy_low_predictions',\n",
       " 'layer1_k_means_2_0',\n",
       " 'layer1_k_means_2_1',\n",
       " 'layer1_k_means_4_0',\n",
       " 'layer1_k_means_4_1',\n",
       " 'layer1_k_means_4_2',\n",
       " 'layer1_k_means_4_3',\n",
       " 'layer1_k_means_8_0',\n",
       " 'layer1_k_means_8_1',\n",
       " 'layer1_k_means_8_2',\n",
       " 'layer1_k_means_8_3',\n",
       " 'layer1_k_means_8_4',\n",
       " 'layer1_k_means_8_5',\n",
       " 'layer1_k_means_8_6',\n",
       " 'layer1_k_means_8_7']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_layer_one_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[0]\ttrain-mlogloss:1.09334+1.46369e-05\ttest-mlogloss:1.09338+3.08765e-05\n",
      "[200]\ttrain-mlogloss:0.714642+0.00145697\ttest-mlogloss:0.724584+0.00286757\n",
      "[400]\ttrain-mlogloss:0.667322+0.00107876\ttest-mlogloss:0.685719+0.00439704\n",
      "[600]\ttrain-mlogloss:0.652947+0.000956845\ttest-mlogloss:0.677757+0.00493822\n",
      "[800]\ttrain-mlogloss:0.64474+0.000976871\ttest-mlogloss:0.674875+0.0051177\n",
      "[1000]\ttrain-mlogloss:0.638542+0.000992185\ttest-mlogloss:0.673386+0.00523967\n",
      "[1200]\ttrain-mlogloss:0.633118+0.000974065\ttest-mlogloss:0.672462+0.00532132\n",
      "[1400]\ttrain-mlogloss:0.62814+0.000962654\ttest-mlogloss:0.671779+0.00539465\n",
      "[1600]\ttrain-mlogloss:0.623502+0.000991192\ttest-mlogloss:0.671327+0.00542241\n",
      "[1800]\ttrain-mlogloss:0.618923+0.00100149\ttest-mlogloss:0.671017+0.00541125\n",
      "[2000]\ttrain-mlogloss:0.61449+0.00103191\ttest-mlogloss:0.670733+0.005445\n",
      "[2200]\ttrain-mlogloss:0.610249+0.00105572\ttest-mlogloss:0.67049+0.00548051\n"
     ]
    }
   ],
   "source": [
    "#PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_representation = pca.fit_transform(train_X_scaled)\n",
    "train_X_with_layer_1.loc[:, pca_columns] = pca_representation\n",
    "\n",
    "#tsne = TSNE(n_components = 2)\n",
    "#tsne_representation = tsne.fit_transform(train_X_scaled[knn_features[:15]][:5000])\n",
    "\n",
    "folds = 5\n",
    "full_length = len(train_X_scaled)\n",
    "entries = int(full_length / folds)\n",
    "full_index = train_X_scaled.index\n",
    "fold_indices = []\n",
    "fold_indices_keep = []\n",
    "fold_indices_throw = []\n",
    "for i in range(folds):\n",
    "    print ('Fold {}'.format(i))\n",
    "    indices = sorted(random.sample(list(full_index), entries))\n",
    "    fold_indices.append(indices)\n",
    "    full_index = full_index.drop(indices)\n",
    "\n",
    "    full_index_keep = np.zeros((full_length), dtype=np.bool)\n",
    "    full_index_throw = np.ones((full_length), dtype=np.bool)\n",
    "    for j in fold_indices[i]:\n",
    "        full_index_keep[j] = True\n",
    "        full_index_throw[j] = False\n",
    "    fold_indices_throw.append(full_index_throw)\n",
    "    fold_indices_keep.append(full_index_keep)\n",
    "    \n",
    "    xtr = train_X_scaled[full_index_throw]\n",
    "    ytr = train_y[full_index_throw]\n",
    "    xte = train_X_scaled[full_index_keep]\n",
    "    \n",
    "    # SPARSE and PRICE\n",
    "    cols = [col for col in list(train_X_scaled.columns) if col.startswith('sparse_')] + ['price_per_bedroom']\n",
    "    dtrain = xgb.DMatrix(data=xtr[cols], label=ytr)\n",
    "    dtest = xgb.DMatrix(data=xte[cols])\n",
    "    bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200)\n",
    "    best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "    print (bst['test-mlogloss-mean'][best_rounds])\n",
    "    bst = xgb.train(params, dtrain, best_rounds)\n",
    "    train_X_with_layer_1.loc[indices, sparse_prediction_cols] = bst.predict(dtest)\n",
    "    \n",
    "    # KNN\n",
    "    knn_features = ['bc_price', 'manager_worths', 'latitude', 'longitude', 'desc_words_length', 'Yday', 'building_worths', 'price_per_bedroom']\n",
    "    for n_neighbors in knn_n_closest:\n",
    "        #knn_dist = DistanceMetric.get_metric('mahalanobis', V=np.eye(len(knn_n_closest)))\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric='manhattan') # metric='manhattan'\n",
    "        knn.fit(train_X_scaled[knn_features][full_index_throw], train_y[full_index_throw])\n",
    "        knn_pred = knn.predict_proba(train_X_scaled[knn_features][full_index_keep])\n",
    "        new_cols = ['layer1_knn_{}_{}'.format(n_neighbors, category) for category in ['high', 'medium', 'low']]\n",
    "        train_X_with_layer_1.loc[indices, new_cols] = knn_pred\n",
    "    \n",
    "    ridge = Ridge()\n",
    "    ridge.fit(xtr, ytr)\n",
    "    ridge_predictions = ridge.predict(xte)\n",
    "    train_X_with_layer_1.loc[indices, 'layer1_ridge_predictions'] = ridge_predictions\n",
    "    \n",
    "    for criterion in ['gini', 'entropy']:\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=25,\n",
    "            criterion=criterion,\n",
    "            max_features='auto',\n",
    "            min_samples_split=20)\n",
    "\n",
    "        rf.fit(xtr, ytr)\n",
    "        rf_predictions = rf.predict_proba(xte)\n",
    "        new_cols = ['layer1_rf_{}_{}_predictions'.format(criterion, category) for category in ['high', 'medium', 'low']]\n",
    "        train_X_with_layer_1.loc[indices, new_cols] = rf_predictions\n",
    "    \n",
    "    for n_clusters in [2, 4, 8]:\n",
    "        k_means = KMeans(n_clusters=n_clusters)\n",
    "        k_means.fit(xtr[knn_features], ytr)\n",
    "        k_means_indices = k_means.predict(xte[knn_features])\n",
    "        new_cols = ['layer1_k_means_{}_{}'.format(n_clusters, index) for index in range(n_clusters)]\n",
    "        train_X_with_layer_1.loc[indices, new_cols] = np.eye(n_clusters)[k_means_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_with_layer_1.to_json('data/train_scaled_with_leaky_and_added_features_cv.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.layers.core import ActivityRegularization\n",
    "def create_model(xtr, layer1=256, layer2=32, activation='tanh', activation2=PReLU, dropout=0.75):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(layer1,\n",
    "                    input_dim=xtr.shape[1],\n",
    "                    init = 'he_normal',\n",
    "                    activation=activation))\n",
    "                    #kernel_regularizer=regularizers.l2(0.01),\n",
    "                    #activity_regularizer=regularizers.activity_l2(0.005)))\n",
    "    #model.add(Dense(64, input_dim=64, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    if activation2 != None:\n",
    "        model.add(activation2())\n",
    "\n",
    "    model.add(Dense(layer2,\n",
    "                    init = 'he_normal',\n",
    "                    activation=activation))\n",
    "                    #activity_regularizer=regularizers.activity_l2(0.005)))\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(dropout))\n",
    "    if activation2 != None:\n",
    "        model.add(activation2())\n",
    "\n",
    "    model.add(Dense(3, init = 'he_normal', activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_nn_with_columns(X, y, columns_to_include, layer1, layer2, activation, activation2, dropout):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    ntrain = int(len(X_scaled) * 0.75)\n",
    "\n",
    "    xtr = X_scaled[columns_to_include][:ntrain]\n",
    "    ytr = y[:ntrain].values\n",
    "    ytr_one_hot = np.eye(3)[ytr]\n",
    "    xte = X_scaled[columns_to_include][ntrain:]\n",
    "    yte = y[ntrain:].values\n",
    "    yte_one_hot = np.eye(3)[yte]\n",
    "\n",
    "    filepath = 'data/nn_weights'\n",
    "\n",
    "    logger = NBatchLogger()\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=40, verbose=0)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "    model = create_model(xtr, layer1, layer2, activation, activation2, dropout)\n",
    "    model.fit(xtr.values, ytr_one_hot, nb_epoch = 1200, batch_size=1000, verbose = 0, validation_data=[xte.values, yte_one_hot], callbacks=[logger, early_stop, checkpoint])\n",
    "    model = create_model(xtr, layer1, layer2, activation, activation2, dropout)\n",
    "    model.load_weights(filepath)\n",
    "    return log_loss(yte_one_hot, model.predict(xte.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actually build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrain = int(len(train_X_with_layer_1) * 0.75)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "xtr = train_X_with_layer_1[:ntrain]\n",
    "xtr = pd.DataFrame(scaler.fit_transform(xtr), columns=xtr.columns)\n",
    "ytr = train_y[:ntrain].values\n",
    "ytr_one_hot = np.eye(3)[ytr]\n",
    "xte = train_X_with_layer_1[ntrain:]\n",
    "xte = pd.DataFrame(scaler.transform(xte), columns=xte.columns)\n",
    "yte = train_y[ntrain:].values\n",
    "yte_one_hot = np.eye(3)[yte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09111+1.37637e-05\ttest-mlogloss:1.09132+3.47597e-05\n",
      "[200]\ttrain-mlogloss:0.572365+0.000966518\ttest-mlogloss:0.606583+0.00289119\n",
      "[400]\ttrain-mlogloss:0.495784+0.00111757\ttest-mlogloss:0.556851+0.00274886\n",
      "[600]\ttrain-mlogloss:0.46142+0.00115805\ttest-mlogloss:0.546969+0.00247487\n",
      "[800]\ttrain-mlogloss:0.435288+0.00122307\ttest-mlogloss:0.543303+0.00232122\n",
      "[1000]\ttrain-mlogloss:0.412388+0.00110456\ttest-mlogloss:0.541574+0.00211891\n",
      "[1200]\ttrain-mlogloss:0.391678+0.0010726\ttest-mlogloss:0.540466+0.00189828\n",
      "[1400]\ttrain-mlogloss:0.372348+0.00112275\ttest-mlogloss:0.539871+0.00179687\n",
      "0.5397984\n",
      "1429\n",
      "0.546284733463\n",
      "CPU times: user 3h 44s, sys: 5min 4s, total: 3h 5min 48s\n",
      "Wall time: 53min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NFOLDS = 5\n",
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "\n",
    "xgb_cols = all_columns + k_means_columns + knn_columns + ridge_columns + pca_columns + rf_columns\n",
    "\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.6,\n",
    "    'subsample':.6,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(data=xtr[xgb_cols], label=ytr)\n",
    "\n",
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=200)\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "print (bst['test-mlogloss-mean'][best_rounds])\n",
    "print (best_rounds)\n",
    "\n",
    "dtest = xgb.DMatrix(data=xte[xgb_cols])\n",
    "bst = xgb.train(params, dtrain, best_rounds)\n",
    "xgb_preds = bst.predict(dtest)\n",
    "print(log_loss(yte_one_hot, xgb_preds))\n",
    "# 0.539725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58044096863\n",
      "CPU times: user 11min 51s, sys: 58.1 s, total: 12min 50s\n",
      "Wall time: 10min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filepath = 'data/nn_weights'\n",
    "\n",
    "nn_cols = all_columns + knn_columns + ridge_columns + k_means_columns\n",
    "\n",
    "logger = NBatchLogger()\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=40, verbose=0)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "model = create_model(xtr[nn_cols])\n",
    "model.fit(xtr[nn_cols].values, ytr_one_hot, nb_epoch = 1200, batch_size=1000, verbose = 0, validation_data=[xte[nn_cols].values, yte_one_hot], callbacks=[logger, early_stop, checkpoint])\n",
    "\n",
    "model = create_model(xtr[nn_cols])\n",
    "model.load_weights(filepath)\n",
    "nn_preds = model.predict(xte[nn_cols].values)\n",
    "print(log_loss(yte_one_hot, nn_preds))\n",
    "# 0.58044096863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6084469079023 (test loss 0.5014715811903182)\n",
      "CPU times: user 10 s, sys: 158 ms, total: 10.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "et_cols = all_columns# + knn_columns + ridge_columns + k_means_columns\n",
    "\n",
    "test = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    max_features='auto',\n",
    "    min_samples_split=50)\n",
    "\n",
    "test.fit(xtr[et_cols], ytr)\n",
    "            \n",
    "et_train_predictions = test.predict_proba(xtr[et_cols])\n",
    "et_predictions = test.predict_proba(xte[et_cols])\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, et_predictions), log_loss(ytr_one_hot, et_train_predictions)))\n",
    "# 0.5989926646715679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5867972591837729 (test loss 0.4744615099148531)\n",
      "CPU times: user 11.2 s, sys: 297 ms, total: 11.5 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_cols = all_columns# + knn_columns + ridge_columns + k_means_columns\n",
    "\n",
    "test = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    max_features='auto',\n",
    "    min_samples_split=50)\n",
    "\n",
    "test.fit(xtr[rf_cols], ytr)\n",
    "            \n",
    "rf_train_predictions = test.predict_proba(xtr[rf_cols])\n",
    "rf_predictions = test.predict_proba(xte[rf_cols])\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, rf_predictions), log_loss(ytr_one_hot, rf_train_predictions)))\n",
    "# 0.5814357939419752"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6644185840766779 (test loss 0.6542689234451893)\n",
      "CPU times: user 5min 38s, sys: 4.82 s, total: 5min 43s\n",
      "Wall time: 5min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=500)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "knn_train_predictions = test.predict_proba(xtr[non_sparse_cols])\n",
    "knn_predictions = test.predict_proba(xte[non_sparse_cols])\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, knn_predictions), log_loss(ytr_one_hot, knn_train_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6542388412538083 (test loss 0.6351798773827041)\n",
      "CPU times: user 49.6 s, sys: 1.34 s, total: 51 s\n",
      "Wall time: 52.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=500)\n",
    "test.fit(xtr[knn_features], ytr)\n",
    "\n",
    "knn_train_predictions = test.predict_proba(xtr[knn_features])\n",
    "knn_predictions = test.predict_proba(xte[knn_features])\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, knn_predictions), log_loss(ytr_one_hot, knn_train_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(log_loss(yte_one_hot, xgb_preds))\n",
    "mixed_preds = (0.75 * xgb_preds) + (0.25 * nn_preds)\n",
    "print(log_loss(yte_one_hot, mixed_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train thing on whether or not each classifier did a good job (minimize distance from correct?)\n",
    "use result to go with the best prediction blended from models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new_vects = ['tf_idf_' + x[7:] for x in sparse_cols]\n",
    "#sparse = processed_train[sparse_cols]\n",
    "#tf_idfs = TfidfTransformer().fit_transform(sparse)\n",
    "#tf_idfs_df = pd.DataFrame(tf_idfs.toarray(), columns=new_vects)\n",
    "#processed_train = processed_train.join(tf_idfs_df, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Lasso Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "las = Lasso()\n",
    "enet = ElasticNet()\n",
    "las.fit(xte, yte)\n",
    "enet.fit(xte, yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "10.478087214\n"
     ]
    }
   ],
   "source": [
    "print (las.score(xte, yte))\n",
    "predictions = las.predict(xte)\n",
    "print (log_loss(yte_one_hot, np.eye(3)[np.round(predictions).astype(int)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "10.478087214\n"
     ]
    }
   ],
   "source": [
    "print (enet.score(xte, yte))\n",
    "predictions = enet.predict(xte)\n",
    "print (log_loss(yte_one_hot, np.eye(3)[np.round(predictions).astype(int)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for coef, column in zip(enet.coef_, train_X.columns):\n",
    "    if coef != 0:\n",
    "        print (column, \": \", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for coef, column in zip(las.coef_, train_X.columns):\n",
    "    if coef != 0:\n",
    "        print (column, \": \", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5843451108603969 (test loss 0.3912286072900344)\n"
     ]
    }
   ],
   "source": [
    "test = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "\n",
    "test.fit(xtr, ytr)\n",
    "            \n",
    "predictions = test.predict_proba(xtr)\n",
    "et_predictions = test.predict_proba(xte)\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, et_predictions), log_loss(ytr_one_hot, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5841850290743527 (test loss 0.3945679429419886)\n"
     ]
    }
   ],
   "source": [
    "test = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "\n",
    "test.fit(xtr, ytr)\n",
    "            \n",
    "predictions = test.predict_proba(xtr)\n",
    "et_ent_predictions = test.predict_proba(xte)\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, et_ent_predictions), log_loss(ytr_one_hot, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5702881670274706 (test loss 0.34260988988913094)\n"
     ]
    }
   ],
   "source": [
    "test = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "\n",
    "test.fit(xtr, ytr)\n",
    "            \n",
    "_predictions = test.predict_proba(xtr)\n",
    "_rf_predictions = test.predict_proba(xte)\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, _rf_predictions), log_loss(ytr_one_hot, _predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5690015004443815 (test loss 0.34470505904504534)\n"
     ]
    }
   ],
   "source": [
    "test = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "\n",
    "test.fit(xtr, ytr)\n",
    "            \n",
    "_predictions = test.predict_proba(xtr)\n",
    "_rf_ent_predictions = test.predict_proba(xte)\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, _rf_ent_predictions), log_loss(ytr_one_hot, _predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56688904952650898"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mixed = (0.01 * et_predictions + 0.01 * et_ent_predictions + 0.49 * rf_predictions + 0.49 * rf_ent_predictions)\n",
    "log_loss(yte_one_hot, pred_mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.076257047822155 (test loss 0.23129250627049086)\n",
      "CPU times: user 3min 15s, sys: 1.48 s, total: 3min 16s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=2)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions = test.predict_proba(xte[non_sparse_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.997161309463761 (test loss 0.4189513500335195)\n",
      "CPU times: user 3min 36s, sys: 1.02 s, total: 3min 37s\n",
      "Wall time: 3min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=5)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions_2 = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions_2 = test.predict_proba(xte[non_sparse_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5474359459916005 (test loss 0.4994428601539661)\n",
      "CPU times: user 3min 48s, sys: 839 ms, total: 3min 49s\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=10)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions_3 = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions_3 = test.predict_proba(xte[non_sparse_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6344317626130038 (test loss 0.6195293255389913)\n",
      "CPU times: user 4min 34s, sys: 1.19 s, total: 4min 35s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = KNeighborsClassifier(n_neighbors=200)\n",
    "test.fit(xtr[non_sparse_cols], ytr)\n",
    "\n",
    "predictions_6 = test.predict_proba(xtr[non_sparse_cols])\n",
    "rf_ent_predictions_6 = test.predict_proba(xte[non_sparse_cols])\n",
    "print ('{} (test loss {})'.format(log_loss(yte_one_hot, rf_ent_predictions_6), log_loss(ytr_one_hot, predictions_6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feat, importance in zip(processed_train[cols].columns, clf.feature_importances_):\n",
    "    if not np.isnan(importance) and importance > 0:\n",
    "        print (feat, \": \", importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn:  0.563593691555\n",
      "ab:  0.566889049527\n",
      "combo:  0.55827286106\n"
     ]
    }
   ],
   "source": [
    "print (\"nn: \", log_loss(yte_one_hot, nn_predictions))\n",
    "print (\"ab: \", log_loss(yte_one_hot, pred_mixed))\n",
    "frac_nn = 0.2\n",
    "averaged_preds = (nn_predictions * frac_nn) + (pred_mixed * (1 - frac_nn))\n",
    "print (\"combo: \", log_loss(yte_one_hot, averaged_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = pd.read_json('data/train_leaky.json')\n",
    "train_y = pd.read_json('data/train_interest.json')[0]\n",
    "scaler = StandardScaler().fit(train_X)\n",
    "train_X = pd.DataFrame(scaler.transform(train_X), columns=columns)\n",
    "test_X = pd.read_json('data/test_leaky.json')\n",
    "test_X = pd.DataFrame(scaler.transform(test_X), columns=columns)\n",
    "listing_id = pd.read_json('data/test_ids.json').values\n",
    "preds = pd.read_csv('data/my_best_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = pd.read_csv('data/my_best_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_clf = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='gini',\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "et_clf.fit(train_X, train_y)          \n",
    "et_predictions = et_clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_clf_ent = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    max_features='auto',\n",
    "    min_samples_split=20)\n",
    "et_clf_ent.fit(train_X, train_y)          \n",
    "et_ent_predictions = et_clf_ent.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='gini',\n",
    "    min_samples_split=20)\n",
    "rf_clf.fit(train_X, train_y)          \n",
    "rf_predictions = rf_clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_clf_ent = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    min_samples_split=20)\n",
    "rf_clf_ent.fit(train_X, train_y)          \n",
    "rf_ent_predictions = rf_clf_ent.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_predictions = model.predict(test_X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_predictions = preds[['high', 'medium', 'low']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_preds = (0.8 * xgb_predictions) + (0.15 * nn_predictions) + (0.05 * et_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_preds_df = pd.DataFrame(new_preds, columns=['high', 'medium', 'low'])\n",
    "new_preds_df['listing_id'] = preds['listing_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_preds_df.to_csv('data/combined_preds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
