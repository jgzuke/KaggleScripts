{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import NaiveBayesClassifier\n",
    "tweets_raw = pd.read_csv(\"data/Sentiment.csv\")\n",
    "sentiments = tweets_raw.loc[:,['sentiment', 'text']]\n",
    "sentiments_pos = sentiments[sentiments.sentiment == 'Positive']\n",
    "sentiments_neg = sentiments[sentiments.sentiment == 'Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jgzuke/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    "\n",
    "word_fd = FreqDist()\n",
    "label_word_fd = ConditionalFreqDist()\n",
    "\n",
    "def word_feats(tweet_full, best_words):\n",
    "    label = tweet_full[0]\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", tweet_full[1])\n",
    "    words_all = [e.lower() for e in words]\n",
    "    words_filtered = [e for e in words_all if (len(e) >= 3) & (e not in stopset)]\n",
    "    \n",
    "    for word in words_filtered:\n",
    "        word_fd[word] += 1\n",
    "        if (label == 'Positive'):\n",
    "            label_word_fd['Positive'][word] += 1\n",
    "        else:\n",
    "            label_word_fd['Negative'][word] += 1\n",
    "    \n",
    "    bigram_finder = BigramCollocationFinder.from_words(words_filtered)\n",
    "    bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 3)\n",
    "    return (dict([(ngram, True) for ngram in itertools.chain(words_filtered, bigrams)]), label)\n",
    "\n",
    "def best_word_feats(tweet_full, best_words):\n",
    "    label = tweet_full[0]\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", tweet_full[1])\n",
    "    words_all = [e.lower() for e in words]\n",
    "    words_filtered = [e for e in words_all if (len(e) >= 3) & (e not in stopset) & (e in best_words)]\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words_all)\n",
    "    bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 3)\n",
    "    return (dict([(ngram, True) for ngram in itertools.chain(words_filtered, bigrams)]), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateFeatures(feature_generator, best_words=None):\n",
    "    neg_features = [feature_generator(tweet_full, best_words) for tweet_full in sentiments_neg.values]\n",
    "    pos_features = [feature_generator(tweet_full, best_words) for tweet_full in sentiments_pos.values]\n",
    "    sentiments_test = neg_features[:400] + pos_features[:400]\n",
    "    sentiments_train = neg_features[400:] + pos_features[400:]\n",
    "    return sentiments_test, sentiments_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk.metrics.scores import precision\n",
    "from nltk.metrics.scores import recall\n",
    "def train_and_test(train, test):\n",
    "    clf = NaiveBayesClassifier.train(train)\n",
    "    print (clf.show_most_informative_features())\n",
    "    \n",
    "    reference_sets = collections.defaultdict(set)\n",
    "    test_sets = collections.defaultdict(set)\n",
    "    for i, features in enumerate(test):\n",
    "        label = features[1]\n",
    "        reference_sets[label].add(i)\n",
    "        predicted = clf.classify(features[0])\n",
    "        test_sets[predicted].add(i)\n",
    "\n",
    "    print (\"accuracy: \" + str(nltk.classify.util.accuracy(clf, sentiments_test)))\n",
    "    print (\"pos_precision: \" + str(nltk.precision(reference_sets['Positive'], test_sets['Positive'])))\n",
    "    print (\"pos_recall: \" + str(nltk.recall(reference_sets['Positive'], test_sets['Positive'])))\n",
    "    print (\"neg_precision: \" + str(nltk.precision(reference_sets['Negative'], test_sets['Negative'])))\n",
    "    print (\"neg_recall: \" + str(nltk.recall(reference_sets['Negative'], test_sets['Negative'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "          donniewahlberg = True           Positi : Negati =     99.6 : 1.0\n",
      "('brought', 'gopdebate') = True           Positi : Negati =     74.9 : 1.0\n",
      "               lrihendry = True           Positi : Negati =     73.9 : 1.0\n",
      "                libertyu = True           Positi : Negati =     54.3 : 1.0\n",
      "  ('candidates', \"i've\") = True           Positi : Negati =     54.3 : 1.0\n",
      "  ('favorite', 'things') = True           Positi : Negati =     54.3 : 1.0\n",
      "           wilberforce91 = True           Positi : Negati =     51.4 : 1.0\n",
      "            kimguilfoyle = True           Positi : Negati =     42.6 : 1.0\n",
      "                 forward = True           Positi : Negati =     40.2 : 1.0\n",
      "               favorites = True           Positi : Negati =     39.7 : 1.0\n",
      "None\n",
      "accuracy: 0.74875\n",
      "pos_precision: 0.7696476964769647\n",
      "pos_recall: 0.71\n",
      "neg_precision: 0.7308584686774942\n",
      "neg_recall: 0.7875\n"
     ]
    }
   ],
   "source": [
    "sentiments_test, sentiments_train = generateFeatures(word_feats)\n",
    "train_and_test(sentiments_train, sentiments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 115437\n"
     ]
    }
   ],
   "source": [
    "pos_word_count = label_word_fd['Positive'].N()\n",
    "neg_word_count = label_word_fd['Negative'].N()\n",
    "total_word_count = pos_word_count + neg_word_count\n",
    " \n",
    "word_scores = {}\n",
    " \n",
    "for word in word_fd:\n",
    "    freq = word_fd[word]\n",
    "    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['Positive'][word], (freq, pos_word_count), total_word_count)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['Negative'][word], (freq, neg_word_count), total_word_count)\n",
    "    word_scores[word] = pos_score + neg_score\n",
    "\n",
    "print ('Total words: ' + str(total_word_count))\n",
    "best_words = sorted(word_scores, key=word_scores.get, reverse=True)[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           ('and', 'am') = True           Positi : Negati =    174.8 : 1.0\n",
      "          donniewahlberg = True           Positi : Negati =     99.6 : 1.0\n",
      "       ('be', 'brought') = True           Positi : Negati =     74.9 : 1.0\n",
      "               lrihendry = True           Positi : Negati =     73.9 : 1.0\n",
      "         ('.', 'thanks') = True           Positi : Negati =     54.3 : 1.0\n",
      "                libertyu = True           Positi : Negati =     54.3 : 1.0\n",
      "           wilberforce91 = True           Positi : Negati =     51.4 : 1.0\n",
      "            kimguilfoyle = True           Positi : Negati =     42.6 : 1.0\n",
      "     ('as', 'president') = True           Positi : Negati =     41.3 : 1.0\n",
      "                 forward = True           Positi : Negati =     40.2 : 1.0\n",
      "None\n",
      "accuracy: 0.795\n",
      "pos_precision: 0.8206521739130435\n",
      "pos_recall: 0.755\n",
      "neg_precision: 0.7731481481481481\n",
      "neg_recall: 0.835\n"
     ]
    }
   ],
   "source": [
    "sentiments_test, sentiments_train = generateFeatures(best_word_feats, best_words)\n",
    "train_and_test(sentiments_train, sentiments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
