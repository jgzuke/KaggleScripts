{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgzuke/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization, LSTM, GRU, TimeDistributedDense\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "import kagglegym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_nan_distributions(features):\n",
    "    cutoff = 800000\n",
    "    missing_values = {}\n",
    "    for row, row_id in zip(features.values, data['id'].values):\n",
    "        key = tuple([not math.isnan(val) for val in row])\n",
    "        if key in missing_values:\n",
    "            missing_values[key] += 1\n",
    "        else:\n",
    "            missing_values[key] = 1\n",
    "\n",
    "    high_frequency_nan_distributions = sorted([(key, missing_values[key]) for key in missing_values], key=lambda key_value: key_value[1])[-500:]\n",
    "    high_frequency_nan_distributions.reverse()\n",
    "    \n",
    "    high_frequency_nan_distributions_filtered = []\n",
    "    for dist, count in high_frequency_nan_distributions:\n",
    "        new = min([sum(int(a != b) for a, b in zip(dist, dist2)) for dist2 in high_frequency_nan_distributions_filtered] + [len(dist)])\n",
    "        if (count * new * new) > cutoff:\n",
    "            high_frequency_nan_distributions_filtered.append(dist)\n",
    "    return np.array(high_frequency_nan_distributions_filtered)\n",
    "\n",
    "def add_nan_distributions(data):\n",
    "    features = data.drop(['id', 'timestamp', 'y'], axis=1)\n",
    "    nan_distributions = get_nan_distributions(features)\n",
    "    best_distributions = np.argmin([[6 * np.sum(np.logical_and(row, dist)) - np.sum(np.logical_or(row, dist)) for dist in nan_distributions] for row in features.isnull().values], axis=1)\n",
    "    nan_features = np.zeros((len(best_distributions), len(nan_distributions)))\n",
    "    nan_features[np.arange(len(best_distributions)), best_distributions] = 1\n",
    "    nan_features = pd.DataFrame(nan_features, columns=['nan_{}'.format(i) for i in range(len(nan_distributions))])\n",
    "    new_data = data.join(nan_features) \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_r2(y_true, y_pred):\n",
    "    u = K.mean(y_true, axis=-1)\n",
    "    u = K.expand_dims(u, y_true.ndim - 1)\n",
    "    u = K.repeat_elements(u, y_true.shape[-1], axis=-1)\n",
    "    r2 = 1 - K.sum(K.square(y_pred - y_true), axis=-1) / K.sum(K.square(y_true - u), axis=-1)\n",
    "    r = (K.sign(r2)*K.sqrt(K.abs(r2)))\n",
    "    return K.clip(r, -1., 1.)\n",
    "\n",
    "def loss_r2_1d(y_true, y_pred):\n",
    "    u = K.mean(y_true)\n",
    "    return K.sum(K.square(y_pred - y_true)) / K.sum(K.square(y_true - u))\n",
    "\n",
    "def loss_r_score(y_true, y_pred):\n",
    "    u = K.mean(y_true)\n",
    "    r2 = 1 - K.sum(K.square(y_pred - y_true)) / K.sum(K.square(y_true - u))\n",
    "    return (K.sign(r2)*K.sqrt(K.abs(r2)))\n",
    "    #return K.clip(r, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 88 ms, sys: 1.08 s, total: 1.16 s\n",
      "Wall time: 2.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = pd.read_hdf('data/train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['                                                                                                              ', '    x x  xxx  xx x xxxx     xxx   xxxx  xx x x   xx xx xxx  xxx   x                                           ', '        x   x                   x           x                  x   x                                          ', '  xxxxx xxxxxxxx xxxxxxxxxx xxxxxxxxxxxxxxxxxxx  xxxxxxxxxx xxxxxxxx x                                        ', '  xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxx xxxxxxxxxx', '  xxxxx x x x  x  x    x  x x xxxxx  xx  x  x x      x x  x x  xx  x x        x               x               ']\n",
      "CPU times: user 36.3 s, sys: 1.77 s, total: 38.1 s\n",
      "Wall time: 39.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TODO dont use 40000, select number based on wanting 6 buckets\n",
    "nan_distributions = get_nan_distributions(data)\n",
    "print (len(nan_distributions))\n",
    "print ([(''.join([' ' if val else 'x' for val in row])) for row in nan_distributions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 s, sys: 7.85 s, total: 30.8 s\n",
      "Wall time: 31.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = pd.read_hdf('data/train.h5')\n",
    "data = data.fillna(data.mean())\n",
    "means, stds = data.mean(), data.std()\n",
    "for column in ['id', 'timestamp', 'y']:\n",
    "    means[column] = 0\n",
    "    stds[column] = 1\n",
    "data = (data - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_back_included = 10\n",
    "num_features = 108 # length of X + 1 extra for y\n",
    "batch_size = 256\n",
    "\n",
    "train_bounds = (0, 400)\n",
    "test_bounds = (400, 700) # 1812\n",
    "epochs = 5\n",
    "train_samples = int(len(data[(data.timestamp >= train_bounds[0]) & (data.timestamp < train_bounds[1])]) / batch_size / 2) * batch_size\n",
    "test_samples = int(len(data[(data.timestamp >= test_bounds[0]) & (data.timestamp < test_bounds[1])]) / batch_size) * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    loss = np.array(history.history['loss_r_score'])\n",
    "    val_loss = np.array(history.history['val_loss_r_score'])\n",
    "    plt.semilogy(np.exp(loss))\n",
    "    plt.semilogy(np.exp(val_loss))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(64, batch_input_shape=[batch_size, samples_back_included, num_features], return_sequences=True, stateful=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GRU(128, return_sequences=False, stateful=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "model.compile(loss=loss_r2_1d,\n",
    "              optimizer='adam',\n",
    "              metrics=[loss_r_score])\n",
    "\n",
    "train_gen = data_generator(data, train_bounds[0], train_bounds[1])\n",
    "test_gen = data_generator(data, test_bounds[0], test_bounds[1])\n",
    "history_128_128 = model.fit_generator(train_gen, samples_per_epoch=train_samples, validation_data=test_gen, nb_val_samples=test_samples, nb_epoch=epochs, verbose=1,)\n",
    "plot(history_128_128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_generator(data_for_id_X, data_for_id_y, id_range, samples_back_included, batch_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    while True:\n",
    "        for j in range(id_range):\n",
    "            for i in range(samples_back_included, len(data_for_id_X[j])):\n",
    "                X.append(data_for_id_X[j][max(i - samples_back_included, 0):i])\n",
    "                y.append(data_for_id_y[j][i-1:i][0][0])\n",
    "                if len(X) == batch_size:\n",
    "                    yield (np.array(X), np.array(y))\n",
    "                    X = []\n",
    "                    y = []\n",
    "\n",
    "def create_and_train_model(env, observation, nan_distributions, batch_size, epochs, passes, samples_back_included, dropout, first_layer_size, second_layer_size):    \n",
    "    data = observation.train\n",
    "    data = add_nan_distributions(data)\n",
    "    data = data.fillna(data.mean())\n",
    "    means, stds = data.mean(), data.std()\n",
    "    nan_rows = [column for column in data.columns if column.startswith('nan_')]\n",
    "    for column in ['id', 'timestamp', 'y'] + nan_rows:\n",
    "        means[column] = 0\n",
    "        stds[column] = 1\n",
    "    data = (data - means) / stds\n",
    "\n",
    "    train_samples = int(len(data) / batch_size / (epochs / passes)) * batch_size\n",
    "                        \n",
    "    ids = np.unique(data.id)\n",
    "    id_range = int(max(ids) - min(ids) + 1)\n",
    "    X_columns = [item for item in data.columns if item not in ('id', 'timestamp', 'y')]\n",
    "    y_columns = ['y']\n",
    "    X_padding = np.zeros((samples_back_included - 1, len(X_columns)), dtype=np.float)\n",
    "    y_padding = np.zeros((samples_back_included - 1, 1), dtype=np.float)\n",
    "\n",
    "    data_for_id_X = [[] for i in range(id_range)]\n",
    "    data_for_id_y = [[] for i in range(id_range)]\n",
    "\n",
    "    for item_id in range(id_range):\n",
    "        data_for_id_X[int(item_id)] = X_padding\n",
    "        data_for_id_y[int(item_id)] = y_padding\n",
    "\n",
    "    for item_id in ids:\n",
    "        data_for_id_X[int(item_id)] = np.concatenate((X_padding, data[data.id == item_id][X_columns].values), axis=0)\n",
    "        data_for_id_y[int(item_id)] = np.concatenate((y_padding, data[data.id == item_id][y_columns].values), axis=0)\n",
    "\n",
    "    data = None\n",
    "    gc.collect()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(GRU(first_layer_size, batch_input_shape=[batch_size, samples_back_included, len(X_columns)], return_sequences=True, stateful=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GRU(second_layer_size, return_sequences=False, stateful=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss=loss_r2_1d,\n",
    "                  optimizer='adam',\n",
    "                  metrics=[loss_r_score])\n",
    "\n",
    "    train_gen = data_generator(data_for_id_X, data_for_id_y, id_range, samples_back_included, batch_size)\n",
    "    history = model.fit_generator(train_gen, samples_per_epoch=train_samples, nb_epoch=epochs, verbose=0)\n",
    "\n",
    "    return means, stds, data_for_id_X, model, history\n",
    "\n",
    "def test_model(env, observation, means, stds, data_for_id_X, model, batch_size, low_y_cut, high_y_cut, samples_back_included):\n",
    "    X_columns = [item for item in observation.features.columns if item not in ('id', 'timestamp')]\n",
    "    X_padding = np.zeros((samples_back_included - 1, len(X_columns)), dtype=np.float)\n",
    "    full_reward = 0\n",
    "    while True:\n",
    "        target = observation.target\n",
    "        features = observation.features\n",
    "        features = features.fillna(0)\n",
    "        features = (features - means) / stds\n",
    "        ids = observation.features['id'].values\n",
    "        for row_id, row in zip(ids, features[X_columns].values):\n",
    "            if row_id >= len(data_for_id_X):\n",
    "                difference = row_id - len(data_for_id_X) + 1\n",
    "                data_for_id_X.extend([X_padding] * difference)\n",
    "            data_for_id_X[row_id] = np.concatenate((data_for_id_X[row_id], [row]), axis=0)\n",
    "\n",
    "        X_to_predict = np.array([data_for_id_X[row_id][-samples_back_included:] for row_id in ids])\n",
    "\n",
    "        # add extra rows to fit batch_size\n",
    "        batches = math.ceil(len(X_to_predict) / batch_size)\n",
    "        extra_predictions = batch_size * batches - len(X_to_predict)\n",
    "        X_to_predict = np.concatenate((X_to_predict, np.zeros((extra_predictions, samples_back_included, 108), dtype=np.int)), axis=0)\n",
    "\n",
    "        target_size = len(target.y)\n",
    "        target.y = model.predict(X_to_predict, batch_size=batch_size)[:target_size].clip(low_y_cut, high_y_cut)\n",
    "        observation, reward, done, info = env.step(target)\n",
    "        if done:\n",
    "            print(\"Finished, reward: \", info[\"public_score\"])\n",
    "            return info[\"public_score\"]\n",
    "        full_reward += reward\n",
    "        if observation.features.timestamp[0] % 200 == 0:\n",
    "            #print(full_reward / 100)\n",
    "            full_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#results_array = pd.read_csv('data/model_results').values\n",
    "#pd.DataFrame(results_array).to_csv('data/model_results', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_params(args):\n",
    "    global results_array\n",
    "    start_time = time.time()\n",
    "    results_dict_key = ', '.join('{0}: {1}'.format(key, args[key]) for key in args)\n",
    "    default_args = {\n",
    "        'batch_size': 256,\n",
    "        'epochs': 4,\n",
    "        'passes': 2,\n",
    "        'low_y_cut': -0.075,\n",
    "        'high_y_cut': 0.075,\n",
    "        'samples_back_included': 8,\n",
    "        'dropout': 0.35,\n",
    "        'first_layer_size': 32,\n",
    "        'second_layer_size': 64\n",
    "    }\n",
    "    env = kagglegym.make()\n",
    "    observation = env.reset()\n",
    "    train_args = dict((arg, args[arg] if arg in args else default_args[arg]) for arg in default_args if arg in ('batch_size', 'epochs', 'passes', 'samples_back_included', 'dropout', 'first_layer_size', 'second_layer_size'))\n",
    "    test_args = dict((arg, args[arg] if arg in args else default_args[arg]) for arg in default_args if arg in ('batch_size', 'samples_back_included', 'low_y_cut', 'high_y_cut'))\n",
    "    means, stds, data_for_id_X, model, history = create_and_train_model(env, observation, **train_args)\n",
    "    score = test_model(env, observation, means, stds, data_for_id_X, model, **test_args)\n",
    "    \n",
    "    results_array = np.append(results_array, [[results_dict_key, score, history, start_time - time.time()]], axis=0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_params_with_nans(args):\n",
    "    global results_array\n",
    "    start_time = time.time()\n",
    "    results_dict_key = ', '.join('{0}: {1}'.format(key, args[key]) for key in args)\n",
    "    default_args = {\n",
    "        'batch_size': 256,\n",
    "        'epochs': 4,\n",
    "        'passes': 2,\n",
    "        'low_y_cut': -0.075,\n",
    "        'high_y_cut': 0.075,\n",
    "        'samples_back_included': 8,\n",
    "        'dropout': 0.35,\n",
    "        'first_layer_size': 32,\n",
    "        'second_layer_size': 64\n",
    "    }\n",
    "    env = kagglegym.make()\n",
    "    observation = env.reset()\n",
    "    train_args = dict((arg, args[arg] if arg in args else default_args[arg]) for arg in default_args if arg in ('batch_size', 'epochs', 'passes', 'samples_back_included', 'dropout', 'first_layer_size', 'second_layer_size'))\n",
    "    test_args = dict((arg, args[arg] if arg in args else default_args[arg]) for arg in default_args if arg in ('batch_size', 'samples_back_included', 'low_y_cut', 'high_y_cut'))\n",
    "    nan_distributions = get_nan_distributions(observation.train)\n",
    "    print (len(nan_distributions))\n",
    "    print ([(''.join([' ' if val else 'x' for val in row])) for row in nan_distributions])\n",
    "    \n",
    "    means, stds, data_for_id_X, model, history = create_and_train_model(env, observation, nan_distributions, **train_args)\n",
    "    score = test_model(env, observation, means, stds, data_for_id_X, model, **test_args)\n",
    "    \n",
    "    results_array = np.append(results_array, [[results_dict_key, score, history, start_time - time.time()]], axis=0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOS\n",
    "- add nans to pandas array\n",
    "- get nan distributions in a way where total num is more constant (iterative?)\n",
    "- do state correctly (prime a bunch of states?)\n",
    "- test optimizers/ltsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgzuke/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/jgzuke/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00428660145471\n",
      "-0.00217862142254\n",
      "-0.00181563727073\n",
      "-0.00116765333644\n",
      "-0.00235118682059\n",
      "-0.00325396125482\n",
      "-0.00175334010547\n",
      "-0.00440014228298\n",
      "0.0017928211677\n",
      "el fin ... -0.162135141475\n"
     ]
    }
   ],
   "source": [
    "import kagglegym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "env = kagglegym.make()\n",
    "o = env.reset()\n",
    "o.train = o.train[:1000]\n",
    "excl = ['id', 'sample', 'y', 'timestamp']\n",
    "col = [c for c in o.train.columns if c not in excl]\n",
    "\n",
    "train = pd.read_hdf('data/train.h5')\n",
    "train = train[col]\n",
    "d_mean= train.median(axis=0)\n",
    "\n",
    "train = o.train[col]\n",
    "n = train.isnull().sum(axis=1)\n",
    "for c in train.columns:\n",
    "    train[c + '_nan_'] = pd.isnull(train[c])\n",
    "    d_mean[c + '_nan_'] = 0\n",
    "train = train.fillna(d_mean)\n",
    "train['znull'] = n\n",
    "n = []\n",
    "\n",
    "rfr = ExtraTreesRegressor(n_estimators=100, max_depth=4, n_jobs=-1, random_state=17, verbose=0)\n",
    "model1 = rfr.fit(train, o.train['y'])\n",
    "train = []\n",
    "\n",
    "#https://www.kaggle.com/bguberfain/two-sigma-financial-modeling/univariate-model-with-clip/run/482189\n",
    "low_y_cut = -0.075\n",
    "high_y_cut = 0.075\n",
    "# 0.075?\n",
    "y_is_above_cut = (o.train.y > high_y_cut)\n",
    "y_is_below_cut = (o.train.y < low_y_cut)\n",
    "y_is_within_cut = (~y_is_above_cut & ~y_is_below_cut)\n",
    "model2 = LinearRegression(n_jobs=-1)\n",
    "model2.fit(np.array(o.train[col].fillna(d_mean).loc[y_is_within_cut, 'technical_20'].values).reshape(-1,1), o.train.loc[y_is_within_cut, 'y'])\n",
    "\n",
    "#https://www.kaggle.com/ymcdull/two-sigma-financial-modeling/ridge-lb-0-0100659\n",
    "ymean_dict = dict(o.train.groupby([\"id\"])[\"y\"].median())\n",
    "\n",
    "while True:\n",
    "    full_reward = 0\n",
    "    test = o.features[col]\n",
    "    n = test.isnull().sum(axis=1)\n",
    "    for c in test.columns:\n",
    "        test[c + '_nan_'] = pd.isnull(test[c])\n",
    "    test = test.fillna(d_mean)\n",
    "    test['znull'] = n\n",
    "    pred = o.target\n",
    "    test2 = np.array(o.features[col].fillna(d_mean)['technical_20'].values).reshape(-1,1)\n",
    "    pred['y'] = (model1.predict(test).clip(low_y_cut, high_y_cut) * 0.65) + (model2.predict(test2).clip(low_y_cut, high_y_cut) * 0.35)\n",
    "    pred['y'] = pred.apply(lambda r: 0.95 * r['y'] + 0.05 * ymean_dict[r['id']] if r['id'] in ymean_dict else r['y'], axis = 1)\n",
    "    pred['y'] = [float(format(x, '.6f')) for x in pred['y']]\n",
    "    o, reward, done, info = env.step(pred)\n",
    "    if done:\n",
    "        print(\"el fin ...\", info[\"public_score\"])\n",
    "        break\n",
    "        \n",
    "    full_reward += reward\n",
    "    if o.features.timestamp[0] % 100 == 0:\n",
    "        print(full_reward / 100)\n",
    "        full_reward = 0\n",
    "# started 11:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
