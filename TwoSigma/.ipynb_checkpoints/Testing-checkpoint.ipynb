{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgzuke/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "import kagglegym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 426 ms, sys: 2.84 s, total: 3.26 s\n",
      "Wall time: 6.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "env = kagglegym.make()\n",
    "observation = env.reset()\n",
    "data = observation.train\n",
    "data_length = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_high_frequency_nan_distributions():\n",
    "    missing_values = {}\n",
    "    for row, row_id in zip(data.drop('y', 1).values, data['id'].values):\n",
    "        key = tuple([not math.isnan(val) for val in row])\n",
    "        if key in missing_values:\n",
    "            missing_values[key] += 1\n",
    "        else:\n",
    "            missing_values[key] = 1\n",
    "\n",
    "    high_frequency_nan_distributions = sorted([(key, missing_values[key]) for key in missing_values], key=lambda key_value: key_value[1])[-500:]\n",
    "    high_frequency_nan_distributions.reverse()\n",
    "    return high_frequency_nan_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_nan_distributions(cutoff, high_frequency_nan_distributions):\n",
    "    high_frequency_nan_distributions_filtered = []\n",
    "    for dist, count in high_frequency_nan_distributions:\n",
    "        new = min([sum(int(a != b) for a, b in zip(dist, dist2)) for dist2 in high_frequency_nan_distributions_filtered] + [len(dist)])\n",
    "        if (count * new * new) > cutoff:\n",
    "            high_frequency_nan_distributions_filtered.append(dist)\n",
    "    return np.array(high_frequency_nan_distributions_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 s, sys: 905 ms, total: 29.5 s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TODO dont use 40000, select number based on wanting 5 buckets\n",
    "high_frequency_nan_distributions = get_high_frequency_nan_distributions()\n",
    "nan_distributions = filter_nan_distributions(500000, high_frequency_nan_distributions)\n",
    "# [(''.join([' ' if val else 'x' for val in row])) for row in nan_distributions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_split_data(data):\n",
    "    has_y = 'y' in data.columns\n",
    "    data_to_use = data.drop('y', 1) if has_y else data\n",
    "    best_distribution = np.argmin([[6 * np.sum(np.logical_and(row, dist)) - np.sum(np.logical_or(row, dist)) for dist in nan_distributions] for row in data_to_use.isnull().values], axis=1)\n",
    "    nan_structures_split = [[best == i for best in best_distribution] for i in range(len(nan_distributions))]\n",
    "    column_splits = [[column for column, included in zip(list(data_to_use.columns), distribution) if included] + (['y'] if has_y else []) for distribution in nan_distributions]\n",
    "    return [data[split][columns] for split, distribution, columns in zip(nan_structures_split, nan_distributions, column_splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 1.1 s, total: 1min 15s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_split = get_split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test_model(model_generator):\n",
    "    clfs = []\n",
    "    total_samples = 0\n",
    "    total_score = 0.\n",
    "    for X_train, y_train, X_test, y_test in data_split_further:\n",
    "        X_train = X_train.fillna(0)\n",
    "        X_test = X_test.fillna(0)\n",
    "\n",
    "        clf = model_generator()\n",
    "        clf.fit(X_train.values, y_train.values)\n",
    "        print ('Trained on {0}, Tested on {1}'.format(len(X_train), len(X_test)))\n",
    "        score = clf.score(X_test.values, y_test.values)\n",
    "        print ('Score ', score)\n",
    "        total_samples += len(X_test)\n",
    "        total_score += score * len(X_test)\n",
    "        clfs.append(clf)\n",
    "    print ('Average Score ', total_score / total_samples)\n",
    "    return clfs\n",
    "\n",
    "def train_model(model_generator, data_split):\n",
    "    clfs = []\n",
    "    for train in data_split:\n",
    "        X_train = train.drop('y', axis=1).fillna(0)\n",
    "        y_train = train['y']\n",
    "        clf = model_generator()\n",
    "        clf.fit(X_train.values, y_train.values)\n",
    "        clfs.append(clf)\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.8 s, sys: 804 ms, total: 1min\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "et_clfs = train_model(lambda: ExtraTreesRegressor(n_estimators=25, max_depth=4, random_state=17, verbose=0), data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "rewards = []\n",
    "while True:\n",
    "    target = observation.target\n",
    "    features_split = get_split_data(observation.features)\n",
    "    for X, clf in zip(features_split, et_clfs):\n",
    "        X = X.fillna(0)\n",
    "        y = clf.predict(X)\n",
    "        for result_id, result in zip(X.id.values, y):\n",
    "            target.loc[observation.target.id == result_id, 'y'] = result\n",
    "    observation, reward, done, info = env.step(target)\n",
    "    if done:\n",
    "        break\n",
    "    rewards.append(reward)\n",
    "    n = n + 1\n",
    "\n",
    "print(info)\n",
    "print(n)\n",
    "print(rewards[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234871\n",
      "66464\n",
      "0.0230805670891\n",
      "465897\n",
      "106243\n",
      "0.00475901717853\n",
      "346164\n",
      "83394\n",
      "0.00891305878331\n",
      "234260\n",
      "72350\n",
      "-0.191136060595\n",
      "41236\n",
      "5302\n",
      "-0.0571993853776\n",
      "42587\n",
      "11988\n",
      "0.0341132880557\n",
      "CPU times: user 19min 3s, sys: 9.3 s, total: 19min 13s\n",
      "Wall time: 5min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_clfs = train_and_test_model(lambda: XGBRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data_split_further = []\n",
    "for data in data_split:\n",
    "    train = data[[row_id in train_ids for row_id in data.id]]\n",
    "    test = data[[row_id in test_ids for row_id in data.id]]\n",
    "    X_train = train.drop('y', axis=1)\n",
    "    X_test = test.drop('y', axis=1)\n",
    "    y_train = train['y']\n",
    "    y_test = test['y']\n",
    "    data_split_further.append([X_train, y_train, X_test, y_test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
